Claro! Para melhorar o modelo XGBoost que voc√™ criou, podemos realizar v√°rias an√°lises e ajustes. Abaixo, vou detalhar algumas estrat√©gias que voc√™ pode seguir para visualizar e melhorar o desempenho do modelo. Vou dividir isso em etapas claras e pr√°ticas.

---

### **1. An√°lise Inicial do Modelo**
Antes de fazer melhorias, √© importante entender o desempenho atual do modelo. Voc√™ j√° est√° calculando a acur√°cia e o F1-Score, o que √© √≥timo. Vamos adicionar mais m√©tricas e visualiza√ß√µes para ter uma vis√£o mais completa.

#### a) **Matriz de Confus√£o**
A matriz de confus√£o ajuda a entender onde o modelo est√° errando (falsos positivos e falsos negativos).

```python
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Calcular a matriz de confus√£o
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot()
plt.show()
```

#### b) **Relat√≥rio de Classifica√ß√£o**
O relat√≥rio de classifica√ß√£o fornece m√©tricas como precis√£o, recall e F1-Score para cada classe.

```python
from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred))
```

#### c) **Curva ROC e AUC**
A curva ROC e a √°rea sob a curva (AUC) s√£o √∫teis para avaliar a capacidade do modelo de distinguir entre as classes.

```python
from sklearn.metrics import roc_curve, auc

# Calcular as probabilidades das classes
y_pred_proba = model.predict_proba(X_test)[:, 1]

# Calcular a curva ROC
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)

# Plotar a curva ROC
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()
```

---

### **2. Ajuste de Hiperpar√¢metros**
O ajuste de hiperpar√¢metros pode melhorar significativamente o desempenho do modelo. Vamos usar t√©cnicas como **Grid Search** ou **Randomized Search** para encontrar os melhores hiperpar√¢metros.

#### a) **Grid Search**
O Grid Search testa todas as combina√ß√µes poss√≠veis de hiperpar√¢metros.

```python
from sklearn.model_selection import GridSearchCV

# Definir os par√¢metros para o Grid Search
param_grid = {
    'learning_rate': [0.001, 0.01, 0.1],
    'max_depth': [3, 4, 5],
    'n_estimators': [100, 200, 500],
    'subsample': [0.7, 0.8, 0.9],
    'colsample_bytree': [0.7, 0.8, 0.9],
    'gamma': [0, 0.1, 0.2],
    'min_child_weight': [1, 5, 10]
}

# Criar o modelo
model = xgb.XGBClassifier(objective='multi:softmax', num_class=2, random_state=42)

# Aplicar o Grid Search
grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='accuracy', cv=3, verbose=1)
grid_search.fit(X_train, y_train)

# Melhores par√¢metros encontrados
print(f'Melhores par√¢metros: {grid_search.best_params_}')

# Treinar o modelo com os melhores par√¢metros
best_model = grid_search.best_estimator_
```

#### b) **Randomized Search**
O Randomized Search testa combina√ß√µes aleat√≥rias de hiperpar√¢metros, o que √© mais eficiente em termos computacionais.

```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform, randint

# Definir os par√¢metros para o Randomized Search
param_dist = {
    'learning_rate': uniform(0.001, 0.1),
    'max_depth': randint(3, 10),
    'n_estimators': randint(100, 1000),
    'subsample': uniform(0.6, 0.4),
    'colsample_bytree': uniform(0.6, 0.4),
    'gamma': uniform(0, 0.5),
    'min_child_weight': randint(1, 10)
}

# Criar o modelo
model = xgb.XGBClassifier(objective='multi:softmax', num_class=2, random_state=42)

# Aplicar o Randomized Search
random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=50, scoring='accuracy', cv=3, verbose=1)
random_search.fit(X_train, y_train)

# Melhores par√¢metros encontrados
print(f'Melhores par√¢metros: {random_search.best_params_}')

# Treinar o modelo com os melhores par√¢metros
best_model = random_search.best_estimator_
```

---

### **3. Valida√ß√£o Cruzada**
A valida√ß√£o cruzada ajuda a garantir que o modelo generalize bem para dados n√£o vistos.

```python
from sklearn.model_selection import cross_val_score

# Aplicar valida√ß√£o cruzada
scores = cross_val_score(best_model, X, y, cv=5, scoring='accuracy')

# M√©dia e desvio padr√£o das pontua√ß√µes
print(f'Acur√°cia m√©dia: {scores.mean():.4f}')
print(f'Desvio padr√£o: {scores.std():.4f}')
```

---

### **4. Feature Engineering**
Melhorar as caracter√≠sticas (features) pode aumentar o desempenho do modelo. Algumas t√©cnicas incluem:
- **Sele√ß√£o de caracter√≠sticas:** Usar m√©todos como `SelectKBest` ou `RFE` (Recursive Feature Elimination).
- **Cria√ß√£o de novas caracter√≠sticas:** Combinar ou transformar caracter√≠sticas existentes para capturar padr√µes adicionais.

---

### **5. Visualiza√ß√£o de Melhorias**
Ap√≥s ajustar o modelo, voc√™ pode comparar as m√©tricas antes e depois das melhorias. Por exemplo:
- Compare a acur√°cia, F1-Score, AUC e matriz de confus√£o antes e depois do ajuste de hiperpar√¢metros.
- Use gr√°ficos para visualizar as diferen√ßas.

---

### **6. Exemplo de C√≥digo Completo**
Aqui est√° um exemplo de como voc√™ pode integrar todas essas etapas:

```python
# Treinar o modelo inicial
model = xgb.XGBClassifier(
    objective='multi:softmax',
    num_class=2,
    eval_metric='mlogloss',
    learning_rate=0.005,
    n_estimators=500,
    max_depth=4,
    subsample=0.7,
    colsample_bytree=0.7,
    gamma=0.2,
    min_child_weight=5,
    reg_alpha=0.1,
    reg_lambda=1.0,
    random_state=42
)
model.fit(X_train, y_train)

# Avaliar o modelo inicial
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))

# Aplicar Grid Search ou Randomized Search
# (Use o c√≥digo de Grid Search ou Randomized Search fornecido acima)

# Avaliar o modelo ajustado
y_pred_best = best_model.predict(X_test)
print(classification_report(y_test, y_pred_best))

# Visualizar a import√¢ncia das caracter√≠sticas
xgb.plot_importance(best_model)
plt.show()
```

---

Com essas an√°lises e ajustes, voc√™ poder√° visualizar e melhorar o desempenho do seu modelo XGBoost. Se precisar de mais ajuda ou tiver d√∫vidas espec√≠ficas, √© s√≥ perguntar! üòä