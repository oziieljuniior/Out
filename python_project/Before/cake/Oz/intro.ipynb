{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oz data read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matriz(num_linhas, array):\n",
    "    \"\"\"\n",
    "    Transforma um array unidimensional em uma matriz organizada por colunas.\n",
    "    \n",
    "    Args:\n",
    "        array (list ou np.array): Lista de números a serem organizados.\n",
    "        num_linhas (int): Número de linhas desejadas na matriz.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Matriz ordenada.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Reshape para matriz (por linha) e depois transpõe para organizar por colunas\n",
    "    matriz = np.array(array).reshape(-1, num_linhas).T\n",
    "    \n",
    "    return matriz # Retorna como lista para melhor legibilidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
       "       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
       "       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
       "       221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
       "       234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
       "       247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n",
       "       260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n",
       "       273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n",
       "       286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n",
       "       299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
       "       312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324,\n",
       "       325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337,\n",
       "       338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350,\n",
       "       351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
       "       364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376,\n",
       "       377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389,\n",
       "       390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402,\n",
       "       403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415,\n",
       "       416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428,\n",
       "       429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441,\n",
       "       442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454,\n",
       "       455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467,\n",
       "       468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480,\n",
       "       481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493,\n",
       "       494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506,\n",
       "       507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519,\n",
       "       520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532,\n",
       "       533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545,\n",
       "       546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558,\n",
       "       559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571,\n",
       "       572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584,\n",
       "       585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597,\n",
       "       598, 599])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array1 = np.arange(600)\n",
    "array1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,  60, 120, 180, 240, 300, 360, 420, 480, 540],\n",
       "       [  1,  61, 121, 181, 241, 301, 361, 421, 481, 541],\n",
       "       [  2,  62, 122, 182, 242, 302, 362, 422, 482, 542],\n",
       "       [  3,  63, 123, 183, 243, 303, 363, 423, 483, 543],\n",
       "       [  4,  64, 124, 184, 244, 304, 364, 424, 484, 544],\n",
       "       [  5,  65, 125, 185, 245, 305, 365, 425, 485, 545],\n",
       "       [  6,  66, 126, 186, 246, 306, 366, 426, 486, 546],\n",
       "       [  7,  67, 127, 187, 247, 307, 367, 427, 487, 547],\n",
       "       [  8,  68, 128, 188, 248, 308, 368, 428, 488, 548],\n",
       "       [  9,  69, 129, 189, 249, 309, 369, 429, 489, 549],\n",
       "       [ 10,  70, 130, 190, 250, 310, 370, 430, 490, 550],\n",
       "       [ 11,  71, 131, 191, 251, 311, 371, 431, 491, 551],\n",
       "       [ 12,  72, 132, 192, 252, 312, 372, 432, 492, 552],\n",
       "       [ 13,  73, 133, 193, 253, 313, 373, 433, 493, 553],\n",
       "       [ 14,  74, 134, 194, 254, 314, 374, 434, 494, 554],\n",
       "       [ 15,  75, 135, 195, 255, 315, 375, 435, 495, 555],\n",
       "       [ 16,  76, 136, 196, 256, 316, 376, 436, 496, 556],\n",
       "       [ 17,  77, 137, 197, 257, 317, 377, 437, 497, 557],\n",
       "       [ 18,  78, 138, 198, 258, 318, 378, 438, 498, 558],\n",
       "       [ 19,  79, 139, 199, 259, 319, 379, 439, 499, 559],\n",
       "       [ 20,  80, 140, 200, 260, 320, 380, 440, 500, 560],\n",
       "       [ 21,  81, 141, 201, 261, 321, 381, 441, 501, 561],\n",
       "       [ 22,  82, 142, 202, 262, 322, 382, 442, 502, 562],\n",
       "       [ 23,  83, 143, 203, 263, 323, 383, 443, 503, 563],\n",
       "       [ 24,  84, 144, 204, 264, 324, 384, 444, 504, 564],\n",
       "       [ 25,  85, 145, 205, 265, 325, 385, 445, 505, 565],\n",
       "       [ 26,  86, 146, 206, 266, 326, 386, 446, 506, 566],\n",
       "       [ 27,  87, 147, 207, 267, 327, 387, 447, 507, 567],\n",
       "       [ 28,  88, 148, 208, 268, 328, 388, 448, 508, 568],\n",
       "       [ 29,  89, 149, 209, 269, 329, 389, 449, 509, 569],\n",
       "       [ 30,  90, 150, 210, 270, 330, 390, 450, 510, 570],\n",
       "       [ 31,  91, 151, 211, 271, 331, 391, 451, 511, 571],\n",
       "       [ 32,  92, 152, 212, 272, 332, 392, 452, 512, 572],\n",
       "       [ 33,  93, 153, 213, 273, 333, 393, 453, 513, 573],\n",
       "       [ 34,  94, 154, 214, 274, 334, 394, 454, 514, 574],\n",
       "       [ 35,  95, 155, 215, 275, 335, 395, 455, 515, 575],\n",
       "       [ 36,  96, 156, 216, 276, 336, 396, 456, 516, 576],\n",
       "       [ 37,  97, 157, 217, 277, 337, 397, 457, 517, 577],\n",
       "       [ 38,  98, 158, 218, 278, 338, 398, 458, 518, 578],\n",
       "       [ 39,  99, 159, 219, 279, 339, 399, 459, 519, 579],\n",
       "       [ 40, 100, 160, 220, 280, 340, 400, 460, 520, 580],\n",
       "       [ 41, 101, 161, 221, 281, 341, 401, 461, 521, 581],\n",
       "       [ 42, 102, 162, 222, 282, 342, 402, 462, 522, 582],\n",
       "       [ 43, 103, 163, 223, 283, 343, 403, 463, 523, 583],\n",
       "       [ 44, 104, 164, 224, 284, 344, 404, 464, 524, 584],\n",
       "       [ 45, 105, 165, 225, 285, 345, 405, 465, 525, 585],\n",
       "       [ 46, 106, 166, 226, 286, 346, 406, 466, 526, 586],\n",
       "       [ 47, 107, 167, 227, 287, 347, 407, 467, 527, 587],\n",
       "       [ 48, 108, 168, 228, 288, 348, 408, 468, 528, 588],\n",
       "       [ 49, 109, 169, 229, 289, 349, 409, 469, 529, 589],\n",
       "       [ 50, 110, 170, 230, 290, 350, 410, 470, 530, 590],\n",
       "       [ 51, 111, 171, 231, 291, 351, 411, 471, 531, 591],\n",
       "       [ 52, 112, 172, 232, 292, 352, 412, 472, 532, 592],\n",
       "       [ 53, 113, 173, 233, 293, 353, 413, 473, 533, 593],\n",
       "       [ 54, 114, 174, 234, 294, 354, 414, 474, 534, 594],\n",
       "       [ 55, 115, 175, 235, 295, 355, 415, 475, 535, 595],\n",
       "       [ 56, 116, 176, 236, 296, 356, 416, 476, 536, 596],\n",
       "       [ 57, 117, 177, 237, 297, 357, 417, 477, 537, 597],\n",
       "       [ 58, 118, 178, 238, 298, 358, 418, 478, 538, 598],\n",
       "       [ 59, 119, 179, 239, 299, 359, 419, 479, 539, 599]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matriz1 = matriz(60, array1)\n",
    "matriz1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['n', 'Entrada', 'Odd', 'P60', 'P120', 'P180', 'P240', 'P300', 'P360',\n",
       "       'P420', 'P480', 'P540', 'P600', 'P660', 'P720', 'P780', 'P840', 'P900',\n",
       "       'P960', 'P1020', 'P1080', 'P1140', 'P1200', 'P1260', 'P1320', 'P1380',\n",
       "       'P1440', 'P1500', 'P1560', 'P1620', 'P1680', 'P1740', 'P1800', 'P1860',\n",
       "       'P1920', 'P1980', 'P1200.1', 'Media Movel', 'Unnamed: 38',\n",
       "       'Unnamed: 39', 'Unnamed: 40', 'Unnamed: 41', 'Unnamed: 42',\n",
       "       'Unnamed: 43', 'Unnamed: 44', 'Unnamed: 45', 'Acertos 60',\n",
       "       'Unnamed: 47', 'Unnamed: 48', 'Unnamed: 49', 'Unnamed: 50',\n",
       "       'Unnamed: 51', 'Unnamed: 52', 'Unnamed: 53', 'Unnamed: 54',\n",
       "       'Unnamed: 55', 'Unnamed: 56', 'Unnamed: 57', 'Unnamed: 58',\n",
       "       'Unnamed: 59', 'Unnamed: 60', 'Unnamed: 61', 'Unnamed: 62',\n",
       "       'Unnamed: 63', 'Unnamed: 64', 'Unnamed: 65', 'Unnamed: 66',\n",
       "       'Unnamed: 67', 'Unnamed: 68', 'Acertos Geral', 'Média Global',\n",
       "       'Unnamed: 71', 'Unnamed: 72', 'Unnamed: 73', 'Unnamed: 74',\n",
       "       'Unnamed: 75', 'Unnamed: 76', 'Unnamed: 77', 'Unnamed: 78',\n",
       "       'Unnamed: 79', 'Unnamed: 80', 'Unnamed: 81', 'Unnamed: 82',\n",
       "       'Unnamed: 83', 'acertos_intervalos'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = pd.read_csv('/home/darkcover/Documentos/Out/dados/Saidas/FUNCOES/DOUBLE - 17_09_s1.csv')\n",
    "data1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        11,6\n",
       "1        2,02\n",
       "2        2,02\n",
       "3        1,54\n",
       "4         2,2\n",
       "        ...  \n",
       "1667        1\n",
       "1668     4,34\n",
       "1669    19,98\n",
       "1670     2,52\n",
       "1671     10,2\n",
       "Name: Entrada, Length: 1672, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array2 = data1['Entrada']\n",
    "array2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6.0,\n",
       " 2.02,\n",
       " 2.02,\n",
       " 1.54,\n",
       " 2.2,\n",
       " 1.51,\n",
       " 6.0,\n",
       " 3.89,\n",
       " 1.02,\n",
       " 1.48,\n",
       " 1.25,\n",
       " 1.07,\n",
       " 1.93,\n",
       " 4.33,\n",
       " 3.59,\n",
       " 3.26,\n",
       " 1.83,\n",
       " 1.6,\n",
       " 2.07,\n",
       " 6.0,\n",
       " 1.21,\n",
       " 5.46,\n",
       " 3.81,\n",
       " 1.3,\n",
       " 1.95,\n",
       " 1.04,\n",
       " 1.28,\n",
       " 1.03,\n",
       " 1.05,\n",
       " 3.39,\n",
       " 1.05,\n",
       " 2.59,\n",
       " 2.53,\n",
       " 6.0,\n",
       " 1.34,\n",
       " 1.56,\n",
       " 1.0,\n",
       " 1.22,\n",
       " 1.05,\n",
       " 6.0,\n",
       " 2.54,\n",
       " 2.44,\n",
       " 2.34,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 1.44,\n",
       " 1.63,\n",
       " 2.57,\n",
       " 1.73,\n",
       " 1.22,\n",
       " 1.34,\n",
       " 1.33,\n",
       " 1.0,\n",
       " 2.87,\n",
       " 2.28,\n",
       " 6.0,\n",
       " 1.0,\n",
       " 1.73,\n",
       " 1.05,\n",
       " 1.52,\n",
       " 5.63,\n",
       " 4.44,\n",
       " 3.91,\n",
       " 6.0,\n",
       " 2.44,\n",
       " 1.43,\n",
       " 1.83,\n",
       " 1.31,\n",
       " 1.61,\n",
       " 1.73,\n",
       " 1.4,\n",
       " 2.12,\n",
       " 1.49,\n",
       " 5.97,\n",
       " 1.12,\n",
       " 1.55,\n",
       " 1.4,\n",
       " 2.62,\n",
       " 2.87,\n",
       " 1.77,\n",
       " 1.02,\n",
       " 1.28,\n",
       " 6.0,\n",
       " 2.18,\n",
       " 1.99,\n",
       " 4.71,\n",
       " 1.01,\n",
       " 1.15,\n",
       " 1.41,\n",
       " 6.0,\n",
       " 1.92,\n",
       " 1.2,\n",
       " 1.0,\n",
       " 2.35,\n",
       " 6.0,\n",
       " 1.24,\n",
       " 4.81,\n",
       " 1.0,\n",
       " 1.27,\n",
       " 1.49,\n",
       " 1.94,\n",
       " 1.98,\n",
       " 5.93,\n",
       " 1.76,\n",
       " 1.9,\n",
       " 1.71,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 2.03,\n",
       " 1.14,\n",
       " 1.29,\n",
       " 6.0,\n",
       " 1.0,\n",
       " 2.21,\n",
       " 3.09,\n",
       " 1.49,\n",
       " 3.34,\n",
       " 1.04,\n",
       " 2.26,\n",
       " 1.21,\n",
       " 1.26,\n",
       " 1.01,\n",
       " 1.21,\n",
       " 3.08,\n",
       " 2.7,\n",
       " 2.3,\n",
       " 2.51,\n",
       " 1.17,\n",
       " 1.02,\n",
       " 4.1,\n",
       " 1.17,\n",
       " 1.61,\n",
       " 6.0,\n",
       " 2.74,\n",
       " 1.67,\n",
       " 5.5,\n",
       " 1.41,\n",
       " 1.13,\n",
       " 1.0,\n",
       " 1.49,\n",
       " 2.3,\n",
       " 1.31,\n",
       " 1.18,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 3.16,\n",
       " 4.08,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 2.16,\n",
       " 5.1,\n",
       " 6.0,\n",
       " 1.65,\n",
       " 1.0,\n",
       " 6.0,\n",
       " 2.12,\n",
       " 1.1,\n",
       " 6.0,\n",
       " 1.13,\n",
       " 1.64,\n",
       " 1.01,\n",
       " 2.11,\n",
       " 1.21,\n",
       " 1.71,\n",
       " 3.63,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 3.42,\n",
       " 6.0,\n",
       " 5.45,\n",
       " 2.62,\n",
       " 3.2,\n",
       " 6.0,\n",
       " 3.1,\n",
       " 3.65,\n",
       " 2.88,\n",
       " 1.74,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 2.47,\n",
       " 6.0,\n",
       " 1.52,\n",
       " 1.5,\n",
       " 2.36,\n",
       " 1.0,\n",
       " 6.0,\n",
       " 1.87,\n",
       " 2.11,\n",
       " 5.53,\n",
       " 1.46,\n",
       " 1.14,\n",
       " 6.0,\n",
       " 1.0,\n",
       " 2.3,\n",
       " 1.41,\n",
       " 3.8,\n",
       " 1.03,\n",
       " 3.41,\n",
       " 6.0,\n",
       " 1.0,\n",
       " 1.39,\n",
       " 2.36,\n",
       " 2.18,\n",
       " 1.3,\n",
       " 1.1,\n",
       " 1.87,\n",
       " 3.76,\n",
       " 1.52,\n",
       " 6.0,\n",
       " 2.08,\n",
       " 4.89,\n",
       " 5.59,\n",
       " 1.18,\n",
       " 6.0,\n",
       " 2.66,\n",
       " 1.65,\n",
       " 1.02,\n",
       " 1.59,\n",
       " 6.0,\n",
       " 1.28,\n",
       " 1.42,\n",
       " 1.08,\n",
       " 2.57,\n",
       " 1.02,\n",
       " 5.11,\n",
       " 1.96,\n",
       " 1.26,\n",
       " 1.17,\n",
       " 1.34,\n",
       " 1.05,\n",
       " 1.18,\n",
       " 2.99,\n",
       " 1.08,\n",
       " 2.19,\n",
       " 1.08,\n",
       " 1.14,\n",
       " 6.0,\n",
       " 2.29,\n",
       " 3.45,\n",
       " 1.07,\n",
       " 6.0,\n",
       " 1.19,\n",
       " 1.05,\n",
       " 1.67,\n",
       " 1.1,\n",
       " 6.0,\n",
       " 1.02,\n",
       " 1.53,\n",
       " 2.38,\n",
       " 1.38,\n",
       " 1.28,\n",
       " 5.21,\n",
       " 6.0,\n",
       " 1.17,\n",
       " 1.06,\n",
       " 3.69,\n",
       " 1.04,\n",
       " 1.54,\n",
       " 1.45,\n",
       " 1.47,\n",
       " 3.41,\n",
       " 6.0,\n",
       " 1.08,\n",
       " 2.15,\n",
       " 1.38,\n",
       " 6.0,\n",
       " 1.02,\n",
       " 4.11,\n",
       " 2.2,\n",
       " 4.17,\n",
       " 2.7,\n",
       " 1.04,\n",
       " 2.14,\n",
       " 1.37,\n",
       " 1.88,\n",
       " 2.85,\n",
       " 6.0,\n",
       " 1.09,\n",
       " 2.08,\n",
       " 4.32,\n",
       " 1.08,\n",
       " 5.52,\n",
       " 2.37,\n",
       " 2.47,\n",
       " 1.66,\n",
       " 1.3,\n",
       " 6.0,\n",
       " 2.6,\n",
       " 6.0,\n",
       " 1.65,\n",
       " 1.0,\n",
       " 1.88,\n",
       " 6.0,\n",
       " 1.51,\n",
       " 3.01,\n",
       " 2.11,\n",
       " 4.35,\n",
       " 1.14,\n",
       " 1.8,\n",
       " 5.07,\n",
       " 6.0,\n",
       " 2.43,\n",
       " 6.0,\n",
       " 1.97,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 3.93,\n",
       " 1.57,\n",
       " 1.31,\n",
       " 5.01,\n",
       " 3.79,\n",
       " 1.34,\n",
       " 5.43,\n",
       " 1.48,\n",
       " 1.26,\n",
       " 1.02,\n",
       " 1.63,\n",
       " 3.74,\n",
       " 1.16,\n",
       " 6.0,\n",
       " 1.29,\n",
       " 2.93,\n",
       " 6.0,\n",
       " 1.01,\n",
       " 6.0,\n",
       " 1.02,\n",
       " 2.73,\n",
       " 2.0,\n",
       " 1.11,\n",
       " 1.49,\n",
       " 4.1,\n",
       " 1.0,\n",
       " 5.61,\n",
       " 1.19,\n",
       " 1.03,\n",
       " 1.22,\n",
       " 1.27,\n",
       " 2.54,\n",
       " 1.36,\n",
       " 1.16,\n",
       " 1.14,\n",
       " 2.85,\n",
       " 1.38,\n",
       " 1.36,\n",
       " 6.0,\n",
       " 1.28,\n",
       " 1.25,\n",
       " 2.47,\n",
       " 3.92,\n",
       " 2.87,\n",
       " 1.09,\n",
       " 1.02,\n",
       " 1.59,\n",
       " 1.72,\n",
       " 1.28,\n",
       " 1.19,\n",
       " 3.48,\n",
       " 1.79,\n",
       " 1.0,\n",
       " 1.08,\n",
       " 3.69,\n",
       " 1.32,\n",
       " 6.0,\n",
       " 1.42,\n",
       " 1.0,\n",
       " 4.26,\n",
       " 1.37,\n",
       " 4.83,\n",
       " 1.65,\n",
       " 1.36,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 2.95,\n",
       " 2.53,\n",
       " 6.0,\n",
       " 1.42,\n",
       " 1.65,\n",
       " 4.93,\n",
       " 1.3,\n",
       " 1.76,\n",
       " 1.21,\n",
       " 4.58,\n",
       " 1.84,\n",
       " 3.43,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 1.61,\n",
       " 1.16,\n",
       " 4.51,\n",
       " 6.0,\n",
       " 2.75,\n",
       " 6.0,\n",
       " 1.65,\n",
       " 1.48,\n",
       " 1.53,\n",
       " 1.09,\n",
       " 1.12,\n",
       " 1.89,\n",
       " 3.44,\n",
       " 5.54,\n",
       " 1.37,\n",
       " 6.0,\n",
       " 3.48,\n",
       " 6.0,\n",
       " 1.07,\n",
       " 1.53,\n",
       " 1.14,\n",
       " 2.26,\n",
       " 1.01,\n",
       " 1.08,\n",
       " 1.01,\n",
       " 1.87,\n",
       " 1.04,\n",
       " 1.87,\n",
       " 3.55,\n",
       " 6.0,\n",
       " 1.05,\n",
       " 2.15,\n",
       " 1.54,\n",
       " 1.06,\n",
       " 1.35,\n",
       " 1.14,\n",
       " 2.66,\n",
       " 2.07,\n",
       " 1.37,\n",
       " 3.49,\n",
       " 1.09,\n",
       " 5.71,\n",
       " 1.85,\n",
       " 6.0,\n",
       " 1.5,\n",
       " 1.58,\n",
       " 2.32,\n",
       " 1.82,\n",
       " 3.7,\n",
       " 1.01,\n",
       " 6.0,\n",
       " 2.69,\n",
       " 3.86,\n",
       " 1.37,\n",
       " 1.74,\n",
       " 2.34,\n",
       " 1.75,\n",
       " 1.95,\n",
       " 1.25,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 4.43,\n",
       " 2.03,\n",
       " 4.2,\n",
       " 6.0,\n",
       " 1.0,\n",
       " 1.08,\n",
       " 1.65,\n",
       " 1.09,\n",
       " 3.88,\n",
       " 3.35,\n",
       " 1.34,\n",
       " 1.64,\n",
       " 2.08,\n",
       " 2.59,\n",
       " 1.1,\n",
       " 1.08,\n",
       " 3.16,\n",
       " 6.0,\n",
       " 1.39,\n",
       " 6.0,\n",
       " 2.74,\n",
       " 3.18,\n",
       " 1.22,\n",
       " 1.12,\n",
       " 1.0,\n",
       " 1.5,\n",
       " 6.0,\n",
       " 1.73,\n",
       " 1.29,\n",
       " 6.0,\n",
       " 1.59,\n",
       " 1.55,\n",
       " 1.21,\n",
       " 1.7,\n",
       " 1.66,\n",
       " 4.99,\n",
       " 1.02,\n",
       " 1.73,\n",
       " 1.19,\n",
       " 2.93,\n",
       " 1.35,\n",
       " 1.07,\n",
       " 2.48,\n",
       " 1.12,\n",
       " 1.28,\n",
       " 1.96,\n",
       " 4.83,\n",
       " 1.2,\n",
       " 1.06,\n",
       " 1.09,\n",
       " 1.04,\n",
       " 2.14,\n",
       " 6.0,\n",
       " 2.36,\n",
       " 1.22,\n",
       " 6.0,\n",
       " 1.19,\n",
       " 1.17,\n",
       " 3.17,\n",
       " 2.85,\n",
       " 1.66,\n",
       " 2.03,\n",
       " 3.91,\n",
       " 4.78,\n",
       " 2.15,\n",
       " 1.23,\n",
       " 1.39,\n",
       " 2.1,\n",
       " 6.0,\n",
       " 2.43,\n",
       " 2.0,\n",
       " 1.12,\n",
       " 1.95,\n",
       " 1.0,\n",
       " 6.0,\n",
       " 1.0,\n",
       " 1.25,\n",
       " 1.58,\n",
       " 1.75,\n",
       " 1.81,\n",
       " 6.0,\n",
       " 3.14,\n",
       " 5.62,\n",
       " 1.37,\n",
       " 6.0,\n",
       " 1.0,\n",
       " 1.51,\n",
       " 1.43,\n",
       " 2.12,\n",
       " 1.33,\n",
       " 1.6,\n",
       " 1.06,\n",
       " 1.05,\n",
       " 1.01,\n",
       " 1.08,\n",
       " 1.67,\n",
       " 1.96,\n",
       " 5.43,\n",
       " 1.21,\n",
       " 1.11,\n",
       " 6.0,\n",
       " 2.13,\n",
       " 1.04,\n",
       " 2.98,\n",
       " 2.78,\n",
       " 1.87,\n",
       " 1.66,\n",
       " 1.09,\n",
       " 1.83,\n",
       " 1.7,\n",
       " 6.0,\n",
       " 1.35,\n",
       " 1.47,\n",
       " 1.01,\n",
       " 1.09,\n",
       " 1.21,\n",
       " 1.87,\n",
       " 1.51,\n",
       " 1.59,\n",
       " 2.17,\n",
       " 4.36,\n",
       " 3.61,\n",
       " 1.3,\n",
       " 1.06,\n",
       " 1.17,\n",
       " 1.11,\n",
       " 4.19,\n",
       " 1.78,\n",
       " 1.71,\n",
       " 1.12,\n",
       " 2.36,\n",
       " 1.71,\n",
       " 6.0,\n",
       " 1.2,\n",
       " 4.29,\n",
       " 1.23,\n",
       " 1.67,\n",
       " 1.02,\n",
       " 2.33,\n",
       " 1.0,\n",
       " 6.0,\n",
       " 3.95,\n",
       " 1.77,\n",
       " 1.33,\n",
       " 6.0,\n",
       " 2.49,\n",
       " 2.24,\n",
       " 6.0,\n",
       " 3.22,\n",
       " 1.49]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array3 = []\n",
    "array4 = []\n",
    "for i in range(600):\n",
    "    odd = array2[i].replace(',','.')\n",
    "    if float(odd) >= 6:\n",
    "        odd = 6\n",
    "    array3.append(float(odd))\n",
    "    if float(odd) >= 3:\n",
    "        corte1 = 1\n",
    "    else:\n",
    "        corte1 = 0\n",
    "    array4.append(corte1)\n",
    "\n",
    "array3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.  , 5.63, 1.21, 6.  , 3.45, 1.8 , 1.  , 2.15, 1.59, 1.6 ],\n",
       "       [2.02, 4.44, 1.26, 2.47, 1.07, 5.07, 1.08, 1.54, 1.55, 1.06],\n",
       "       [2.02, 3.91, 1.01, 6.  , 6.  , 6.  , 3.69, 1.06, 1.21, 1.05],\n",
       "       [1.54, 6.  , 1.21, 1.52, 1.19, 2.43, 1.32, 1.35, 1.7 , 1.01],\n",
       "       [2.2 , 2.44, 3.08, 1.5 , 1.05, 6.  , 6.  , 1.14, 1.66, 1.08],\n",
       "       [1.51, 1.43, 2.7 , 2.36, 1.67, 1.97, 1.42, 2.66, 4.99, 1.67],\n",
       "       [6.  , 1.83, 2.3 , 1.  , 1.1 , 6.  , 1.  , 2.07, 1.02, 1.96],\n",
       "       [3.89, 1.31, 2.51, 6.  , 6.  , 6.  , 4.26, 1.37, 1.73, 5.43],\n",
       "       [1.02, 1.61, 1.17, 1.87, 1.02, 3.93, 1.37, 3.49, 1.19, 1.21],\n",
       "       [1.48, 1.73, 1.02, 2.11, 1.53, 1.57, 4.83, 1.09, 2.93, 1.11],\n",
       "       [1.25, 1.4 , 4.1 , 5.53, 2.38, 1.31, 1.65, 5.71, 1.35, 6.  ],\n",
       "       [1.07, 2.12, 1.17, 1.46, 1.38, 5.01, 1.36, 1.85, 1.07, 2.13],\n",
       "       [1.93, 1.49, 1.61, 1.14, 1.28, 3.79, 6.  , 6.  , 2.48, 1.04],\n",
       "       [4.33, 5.97, 6.  , 6.  , 5.21, 1.34, 6.  , 1.5 , 1.12, 2.98],\n",
       "       [3.59, 1.12, 2.74, 1.  , 6.  , 5.43, 6.  , 1.58, 1.28, 2.78],\n",
       "       [3.26, 1.55, 1.67, 2.3 , 1.17, 1.48, 2.95, 2.32, 1.96, 1.87],\n",
       "       [1.83, 1.4 , 5.5 , 1.41, 1.06, 1.26, 2.53, 1.82, 4.83, 1.66],\n",
       "       [1.6 , 2.62, 1.41, 3.8 , 3.69, 1.02, 6.  , 3.7 , 1.2 , 1.09],\n",
       "       [2.07, 2.87, 1.13, 1.03, 1.04, 1.63, 1.42, 1.01, 1.06, 1.83],\n",
       "       [6.  , 1.77, 1.  , 3.41, 1.54, 3.74, 1.65, 6.  , 1.09, 1.7 ],\n",
       "       [1.21, 1.02, 1.49, 6.  , 1.45, 1.16, 4.93, 2.69, 1.04, 6.  ],\n",
       "       [5.46, 1.28, 2.3 , 1.  , 1.47, 6.  , 1.3 , 3.86, 2.14, 1.35],\n",
       "       [3.81, 6.  , 1.31, 1.39, 3.41, 1.29, 1.76, 1.37, 6.  , 1.47],\n",
       "       [1.3 , 2.18, 1.18, 2.36, 6.  , 2.93, 1.21, 1.74, 2.36, 1.01],\n",
       "       [1.95, 1.99, 6.  , 2.18, 1.08, 6.  , 4.58, 2.34, 1.22, 1.09],\n",
       "       [1.04, 4.71, 6.  , 1.3 , 2.15, 1.01, 1.84, 1.75, 6.  , 1.21],\n",
       "       [1.28, 1.01, 3.16, 1.1 , 1.38, 6.  , 3.43, 1.95, 1.19, 1.87],\n",
       "       [1.03, 1.15, 4.08, 1.87, 6.  , 1.02, 6.  , 1.25, 1.17, 1.51],\n",
       "       [1.05, 1.41, 6.  , 3.76, 1.02, 2.73, 6.  , 6.  , 3.17, 1.59],\n",
       "       [3.39, 6.  , 6.  , 1.52, 4.11, 2.  , 1.61, 6.  , 2.85, 2.17],\n",
       "       [1.05, 1.92, 6.  , 6.  , 2.2 , 1.11, 1.16, 4.43, 1.66, 4.36],\n",
       "       [2.59, 1.2 , 2.16, 2.08, 4.17, 1.49, 4.51, 2.03, 2.03, 3.61],\n",
       "       [2.53, 1.  , 5.1 , 4.89, 2.7 , 4.1 , 6.  , 4.2 , 3.91, 1.3 ],\n",
       "       [6.  , 2.35, 6.  , 5.59, 1.04, 1.  , 2.75, 6.  , 4.78, 1.06],\n",
       "       [1.34, 6.  , 1.65, 1.18, 2.14, 5.61, 6.  , 1.  , 2.15, 1.17],\n",
       "       [1.56, 1.24, 1.  , 6.  , 1.37, 1.19, 1.65, 1.08, 1.23, 1.11],\n",
       "       [1.  , 4.81, 6.  , 2.66, 1.88, 1.03, 1.48, 1.65, 1.39, 4.19],\n",
       "       [1.22, 1.  , 2.12, 1.65, 2.85, 1.22, 1.53, 1.09, 2.1 , 1.78],\n",
       "       [1.05, 1.27, 1.1 , 1.02, 6.  , 1.27, 1.09, 3.88, 6.  , 1.71],\n",
       "       [6.  , 1.49, 6.  , 1.59, 1.09, 2.54, 1.12, 3.35, 2.43, 1.12],\n",
       "       [2.54, 1.94, 1.13, 6.  , 2.08, 1.36, 1.89, 1.34, 2.  , 2.36],\n",
       "       [2.44, 1.98, 1.64, 1.28, 4.32, 1.16, 3.44, 1.64, 1.12, 1.71],\n",
       "       [2.34, 5.93, 1.01, 1.42, 1.08, 1.14, 5.54, 2.08, 1.95, 6.  ],\n",
       "       [6.  , 1.76, 2.11, 1.08, 5.52, 2.85, 1.37, 2.59, 1.  , 1.2 ],\n",
       "       [6.  , 1.9 , 1.21, 2.57, 2.37, 1.38, 6.  , 1.1 , 6.  , 4.29],\n",
       "       [1.44, 1.71, 1.71, 1.02, 2.47, 1.36, 3.48, 1.08, 1.  , 1.23],\n",
       "       [1.63, 6.  , 3.63, 5.11, 1.66, 6.  , 6.  , 3.16, 1.25, 1.67],\n",
       "       [2.57, 6.  , 6.  , 1.96, 1.3 , 1.28, 1.07, 6.  , 1.58, 1.02],\n",
       "       [1.73, 6.  , 6.  , 1.26, 6.  , 1.25, 1.53, 1.39, 1.75, 2.33],\n",
       "       [1.22, 2.03, 3.42, 1.17, 2.6 , 2.47, 1.14, 6.  , 1.81, 1.  ],\n",
       "       [1.34, 1.14, 6.  , 1.34, 6.  , 3.92, 2.26, 2.74, 6.  , 6.  ],\n",
       "       [1.33, 1.29, 5.45, 1.05, 1.65, 2.87, 1.01, 3.18, 3.14, 3.95],\n",
       "       [1.  , 6.  , 2.62, 1.18, 1.  , 1.09, 1.08, 1.22, 5.62, 1.77],\n",
       "       [2.87, 1.  , 3.2 , 2.99, 1.88, 1.02, 1.01, 1.12, 1.37, 1.33],\n",
       "       [2.28, 2.21, 6.  , 1.08, 6.  , 1.59, 1.87, 1.  , 6.  , 6.  ],\n",
       "       [6.  , 3.09, 3.1 , 2.19, 1.51, 1.72, 1.04, 1.5 , 1.  , 2.49],\n",
       "       [1.  , 1.49, 3.65, 1.08, 3.01, 1.28, 1.87, 6.  , 1.51, 2.24],\n",
       "       [1.73, 3.34, 2.88, 1.14, 2.11, 1.19, 3.55, 1.73, 1.43, 6.  ],\n",
       "       [1.05, 1.04, 1.74, 6.  , 4.35, 3.48, 6.  , 1.29, 2.12, 3.22],\n",
       "       [1.52, 2.26, 6.  , 2.29, 1.14, 1.79, 1.05, 6.  , 1.33, 1.49]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matriz2 = matriz(60, array3)\n",
    "matriz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 1, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 1, 1, 1, 1, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 1, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [1, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 1, 1, 1, 1, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 1, 0, 0, 0, 1, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 1, 1, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 0, 1, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 1, 1, 1, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 1, 1, 0, 1, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 1, 0, 1, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 1, 0, 0, 1],\n",
       "       [1, 0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       "       [1, 1, 0, 0, 1, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 1, 1, 0, 0, 0],\n",
       "       [0, 1, 1, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 1, 0, 0, 1, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 1, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 1, 0, 0, 1, 1, 1, 0],\n",
       "       [1, 1, 1, 0, 1, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 1, 1, 0, 0, 0, 1, 0, 1],\n",
       "       [0, 0, 0, 0, 1, 0, 1, 0, 0, 1],\n",
       "       [0, 0, 1, 1, 0, 1, 1, 1, 1, 0],\n",
       "       [1, 0, 1, 1, 0, 0, 0, 1, 1, 0],\n",
       "       [0, 1, 0, 0, 0, 1, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 1, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 1, 1, 0],\n",
       "       [1, 0, 1, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 1, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 1, 0, 0, 1],\n",
       "       [1, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 1, 0, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 1, 1, 1, 0, 1, 1, 1, 0, 0],\n",
       "       [0, 1, 1, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 1, 1, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 1, 0, 1, 1, 0, 0, 1, 1],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 1, 1, 1],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 1, 0, 0, 0, 1, 1],\n",
       "       [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 1, 0, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 1, 0, 0, 1],\n",
       "       [0, 0, 0, 1, 1, 1, 1, 0, 0, 1],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matriz3 = matriz(60,array4)\n",
    "matriz3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 18,\n",
       " 17,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 15,\n",
       " 15,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 13,\n",
       " 13,\n",
       " 12,\n",
       " 11,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 12,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 12,\n",
       " 12,\n",
       " 13,\n",
       " 13,\n",
       " 14,\n",
       " 14,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 16,\n",
       " 16,\n",
       " 15,\n",
       " 16,\n",
       " 16,\n",
       " 17,\n",
       " 17,\n",
       " 16,\n",
       " 15,\n",
       " 14,\n",
       " 13,\n",
       " 13,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 15,\n",
       " 15,\n",
       " 14,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 20,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 20,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 22,\n",
       " 22,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 23,\n",
       " 23,\n",
       " 24,\n",
       " 24,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 27,\n",
       " 28,\n",
       " 27,\n",
       " 27,\n",
       " 27,\n",
       " 27,\n",
       " 28,\n",
       " 28,\n",
       " 27,\n",
       " 28,\n",
       " 28,\n",
       " 27,\n",
       " 28,\n",
       " 28,\n",
       " 27,\n",
       " 27,\n",
       " 28,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 29,\n",
       " 28,\n",
       " 27,\n",
       " 26,\n",
       " 25,\n",
       " 25,\n",
       " 24,\n",
       " 25,\n",
       " 24,\n",
       " 24,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 24,\n",
       " 24,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 24,\n",
       " 24,\n",
       " 23,\n",
       " 22,\n",
       " 21,\n",
       " 20,\n",
       " 20,\n",
       " 19,\n",
       " 18,\n",
       " 17,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 15,\n",
       " 16,\n",
       " 15,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 15,\n",
       " 16,\n",
       " 16,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 16,\n",
       " 15,\n",
       " 16,\n",
       " 15,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 15,\n",
       " 15,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 15,\n",
       " 14,\n",
       " 14,\n",
       " 15,\n",
       " 15,\n",
       " 16,\n",
       " 16,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 16,\n",
       " 16,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 18,\n",
       " 18,\n",
       " 19,\n",
       " 18,\n",
       " 19,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 19,\n",
       " 19,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 23,\n",
       " 23,\n",
       " 22,\n",
       " 23,\n",
       " 23,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 23,\n",
       " 23,\n",
       " 23,\n",
       " 22,\n",
       " 22,\n",
       " 23,\n",
       " 23,\n",
       " 23,\n",
       " 23,\n",
       " 22,\n",
       " 22,\n",
       " 21,\n",
       " 21,\n",
       " 22,\n",
       " 22,\n",
       " 23,\n",
       " 23,\n",
       " 23,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 21,\n",
       " 21,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 21,\n",
       " 20,\n",
       " 20,\n",
       " 19,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 19,\n",
       " 19,\n",
       " 18,\n",
       " 18,\n",
       " 17,\n",
       " 18,\n",
       " 18,\n",
       " 17,\n",
       " 16,\n",
       " 17,\n",
       " 16,\n",
       " 17,\n",
       " 16,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 16,\n",
       " 15,\n",
       " 14,\n",
       " 15,\n",
       " 15,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 17,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 15,\n",
       " 16,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 19,\n",
       " 18,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 21,\n",
       " 22,\n",
       " 22,\n",
       " 23,\n",
       " 23,\n",
       " 23,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 23,\n",
       " 23,\n",
       " 23,\n",
       " 22,\n",
       " 22,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 21,\n",
       " 20,\n",
       " 20,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 18,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 20,\n",
       " 20,\n",
       " 19,\n",
       " 19,\n",
       " 18,\n",
       " 17,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 18,\n",
       " 17,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 19,\n",
       " 18,\n",
       " 18,\n",
       " 17,\n",
       " 16,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 17,\n",
       " 18,\n",
       " 18,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 18,\n",
       " 18,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 20,\n",
       " 20,\n",
       " 19,\n",
       " 19,\n",
       " 18,\n",
       " 18,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 16,\n",
       " 16,\n",
       " 15,\n",
       " 15,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 17,\n",
       " 17,\n",
       " 16,\n",
       " 16,\n",
       " 15,\n",
       " 15,\n",
       " 14,\n",
       " 14,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 15,\n",
       " 14,\n",
       " 13,\n",
       " 13,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 14,\n",
       " 15,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 14,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 12,\n",
       " 12,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 12,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 12,\n",
       " 11,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 10,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 12]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array5 = []\n",
    "for i in range(len(array4) - 1):\n",
    "    if i >= 59:\n",
    "        order = sum(array4[i - 59: i])\n",
    "        array5.append(order)\n",
    "array5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[15, 17, 25, 16, 19, 18, 23, 18, 13],\n",
       "       [14, 16, 26, 15, 18, 18, 23, 19, 13],\n",
       "       [15, 15, 27, 16, 18, 17, 23, 19, 13],\n",
       "       [16, 14, 27, 15, 18, 16, 22, 19, 13],\n",
       "       [17, 13, 28, 16, 19, 17, 22, 19, 13],\n",
       "       [18, 13, 27, 16, 19, 16, 21, 19, 13],\n",
       "       [18, 14, 27, 16, 20, 17, 21, 19, 12],\n",
       "       [17, 14, 27, 16, 20, 16, 21, 20, 12],\n",
       "       [16, 14, 27, 15, 20, 15, 20, 20, 12],\n",
       "       [16, 14, 28, 16, 21, 15, 20, 19, 13],\n",
       "       [16, 14, 28, 16, 22, 15, 20, 19, 13],\n",
       "       [16, 14, 27, 15, 22, 16, 20, 18, 13],\n",
       "       [16, 15, 28, 15, 22, 15, 21, 18, 14],\n",
       "       [16, 15, 28, 15, 23, 14, 20, 17, 14],\n",
       "       [15, 14, 27, 14, 23, 15, 20, 17, 14],\n",
       "       [15, 15, 28, 15, 22, 15, 19, 17, 14],\n",
       "       [14, 15, 28, 16, 23, 16, 19, 17, 14],\n",
       "       [14, 15, 27, 16, 23, 16, 19, 17, 13],\n",
       "       [14, 16, 27, 15, 22, 16, 18, 17, 13],\n",
       "       [14, 16, 28, 16, 22, 17, 19, 17, 13],\n",
       "       [13, 16, 28, 15, 22, 16, 19, 16, 13],\n",
       "       [13, 16, 29, 14, 23, 16, 19, 16, 13],\n",
       "       [12, 16, 30, 14, 23, 16, 19, 15, 14],\n",
       "       [11, 15, 30, 14, 23, 16, 20, 15, 13],\n",
       "       [12, 15, 30, 15, 22, 16, 20, 16, 13],\n",
       "       [12, 15, 29, 16, 22, 15, 19, 16, 13],\n",
       "       [12, 15, 28, 16, 23, 16, 19, 16, 12],\n",
       "       [13, 16, 27, 16, 23, 15, 18, 17, 12],\n",
       "       [13, 17, 26, 16, 23, 16, 17, 17, 12],\n",
       "       [13, 18, 25, 16, 23, 17, 16, 16, 11],\n",
       "       [12, 18, 25, 16, 22, 18, 17, 16, 11],\n",
       "       [13, 19, 24, 16, 22, 18, 18, 15, 11],\n",
       "       [13, 20, 25, 16, 21, 18, 18, 15, 12],\n",
       "       [13, 20, 24, 16, 21, 18, 17, 14, 12],\n",
       "       [12, 21, 24, 15, 22, 19, 18, 14, 11],\n",
       "       [12, 21, 25, 15, 22, 18, 18, 15, 11],\n",
       "       [13, 21, 25, 14, 23, 19, 18, 15, 11],\n",
       "       [13, 20, 25, 14, 23, 19, 18, 15, 11],\n",
       "       [14, 21, 25, 14, 23, 19, 18, 15, 12],\n",
       "       [14, 21, 25, 14, 22, 19, 18, 14, 11],\n",
       "       [13, 21, 24, 15, 22, 19, 19, 14, 11],\n",
       "       [13, 22, 24, 14, 22, 19, 20, 14, 11],\n",
       "       [13, 22, 25, 14, 21, 19, 19, 14, 11],\n",
       "       [13, 21, 25, 15, 21, 20, 18, 14, 11],\n",
       "       [13, 21, 25, 15, 20, 21, 18, 14, 12],\n",
       "       [12, 21, 25, 16, 20, 21, 17, 14, 11],\n",
       "       [12, 21, 25, 16, 20, 22, 16, 15, 12],\n",
       "       [12, 20, 24, 15, 20, 22, 15, 14, 12],\n",
       "       [13, 20, 24, 15, 21, 23, 16, 13, 12],\n",
       "       [14, 20, 23, 15, 20, 23, 17, 13, 12],\n",
       "       [15, 21, 22, 16, 20, 23, 17, 12, 12],\n",
       "       [15, 22, 21, 16, 19, 22, 18, 12, 11],\n",
       "       [15, 23, 20, 17, 20, 22, 18, 12, 11],\n",
       "       [15, 23, 20, 17, 20, 22, 19, 13, 11],\n",
       "       [16, 23, 19, 17, 20, 22, 19, 14, 11],\n",
       "       [16, 24, 18, 17, 19, 22, 19, 14, 10],\n",
       "       [15, 24, 17, 18, 19, 22, 19, 15, 11],\n",
       "       [16, 25, 16, 18, 18, 22, 19, 14, 11],\n",
       "       [16, 25, 16, 19, 18, 22, 19, 14, 11],\n",
       "       [17, 25, 16, 18, 17, 22, 18, 14, 12]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matriz4 = matriz(60, array5)\n",
    "matriz4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indicador de futura função:\n",
    "import numpy as np\n",
    "\n",
    "def calculate_means(array4):\n",
    "    \"\"\"\n",
    "    Calcula a média dos elementos de array4 em janelas deslizantes de 59 elementos.\n",
    "\n",
    "    Args:\n",
    "        array4 (list): Lista de inteiros (0 ou 1).\n",
    "\n",
    "    Returns:\n",
    "        list: Lista com a média dos elementos em janelas de 59 elementos.\n",
    "    \"\"\"\n",
    "    array6 = []\n",
    "    array7 = []\n",
    "    for i in range(len(array4) - 1):\n",
    "        array6.append(array4[i])\n",
    "        if i >= 59:\n",
    "            order = float(np.mean(array6))\n",
    "            array7.append(order)\n",
    "    return array7\n",
    "\n",
    "# Exemplo de uso:\n",
    "# array4 = [0, 1, 0, 1, ...]  # Supondo que array4 tenha elementos suficientes\n",
    "# array7 = calculate_means(array4)\n",
    "\n",
    "# Teste unitário básico\n",
    "def test_calculate_means():\n",
    "    array4 = [1] * 60  # Lista com 60 elementos, todos iguais a 1\n",
    "    expected_output = [1.0]  # A média dos primeiros 59 elementos é 1.0\n",
    "    array7 = calculate_means(array4)\n",
    "    assert array7 == expected_output, \"Teste falhou!\"\n",
    "\n",
    "# Executar o teste\n",
    "test_calculate_means()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.25,\n",
       " 0.26229508196721313,\n",
       " 0.27419354838709675,\n",
       " 0.2857142857142857,\n",
       " 0.296875,\n",
       " 0.2923076923076923,\n",
       " 0.2878787878787879,\n",
       " 0.2835820895522388,\n",
       " 0.27941176470588236,\n",
       " 0.2753623188405797,\n",
       " 0.2714285714285714,\n",
       " 0.2676056338028169,\n",
       " 0.2638888888888889,\n",
       " 0.2602739726027397,\n",
       " 0.2702702702702703,\n",
       " 0.26666666666666666,\n",
       " 0.2631578947368421,\n",
       " 0.2597402597402597,\n",
       " 0.2564102564102564,\n",
       " 0.25316455696202533,\n",
       " 0.25,\n",
       " 0.24691358024691357,\n",
       " 0.24390243902439024,\n",
       " 0.25301204819277107,\n",
       " 0.25,\n",
       " 0.24705882352941178,\n",
       " 0.2558139534883721,\n",
       " 0.25287356321839083,\n",
       " 0.25,\n",
       " 0.24719101123595505,\n",
       " 0.25555555555555554,\n",
       " 0.25274725274725274,\n",
       " 0.25,\n",
       " 0.24731182795698925,\n",
       " 0.24468085106382978,\n",
       " 0.25263157894736843,\n",
       " 0.25,\n",
       " 0.25773195876288657,\n",
       " 0.25510204081632654,\n",
       " 0.25252525252525254,\n",
       " 0.25,\n",
       " 0.24752475247524752,\n",
       " 0.24509803921568626,\n",
       " 0.2524271844660194,\n",
       " 0.25,\n",
       " 0.24761904761904763,\n",
       " 0.24528301886792453,\n",
       " 0.2523364485981308,\n",
       " 0.25925925925925924,\n",
       " 0.26605504587155965,\n",
       " 0.2636363636363636,\n",
       " 0.26126126126126126,\n",
       " 0.25892857142857145,\n",
       " 0.26548672566371684,\n",
       " 0.2631578947368421,\n",
       " 0.2608695652173913,\n",
       " 0.2672413793103448,\n",
       " 0.26495726495726496,\n",
       " 0.2711864406779661,\n",
       " 0.2689075630252101,\n",
       " 0.26666666666666666,\n",
       " 0.2644628099173554,\n",
       " 0.26229508196721313,\n",
       " 0.2601626016260163,\n",
       " 0.25806451612903225,\n",
       " 0.264,\n",
       " 0.2619047619047619,\n",
       " 0.25984251968503935,\n",
       " 0.2578125,\n",
       " 0.2558139534883721,\n",
       " 0.25384615384615383,\n",
       " 0.2595419847328244,\n",
       " 0.25757575757575757,\n",
       " 0.2556390977443609,\n",
       " 0.26119402985074625,\n",
       " 0.25925925925925924,\n",
       " 0.25735294117647056,\n",
       " 0.26277372262773724,\n",
       " 0.2608695652173913,\n",
       " 0.2589928057553957,\n",
       " 0.2571428571428571,\n",
       " 0.2553191489361702,\n",
       " 0.2535211267605634,\n",
       " 0.2517482517482518,\n",
       " 0.25,\n",
       " 0.25517241379310346,\n",
       " 0.2602739726027397,\n",
       " 0.2653061224489796,\n",
       " 0.2702702702702703,\n",
       " 0.2751677852348993,\n",
       " 0.28,\n",
       " 0.2847682119205298,\n",
       " 0.28289473684210525,\n",
       " 0.2875816993464052,\n",
       " 0.2922077922077922,\n",
       " 0.2903225806451613,\n",
       " 0.28846153846153844,\n",
       " 0.2929936305732484,\n",
       " 0.2911392405063291,\n",
       " 0.2893081761006289,\n",
       " 0.29375,\n",
       " 0.2919254658385093,\n",
       " 0.29012345679012347,\n",
       " 0.2883435582822086,\n",
       " 0.2865853658536585,\n",
       " 0.28484848484848485,\n",
       " 0.28313253012048195,\n",
       " 0.2874251497005988,\n",
       " 0.2916666666666667,\n",
       " 0.2958579881656805,\n",
       " 0.3,\n",
       " 0.30409356725146197,\n",
       " 0.3081395348837209,\n",
       " 0.3063583815028902,\n",
       " 0.3103448275862069,\n",
       " 0.3142857142857143,\n",
       " 0.3181818181818182,\n",
       " 0.3220338983050847,\n",
       " 0.3202247191011236,\n",
       " 0.31843575418994413,\n",
       " 0.32222222222222224,\n",
       " 0.3259668508287293,\n",
       " 0.3241758241758242,\n",
       " 0.32786885245901637,\n",
       " 0.32608695652173914,\n",
       " 0.32432432432432434,\n",
       " 0.3225806451612903,\n",
       " 0.32085561497326204,\n",
       " 0.324468085106383,\n",
       " 0.32275132275132273,\n",
       " 0.32105263157894737,\n",
       " 0.32460732984293195,\n",
       " 0.3229166666666667,\n",
       " 0.32124352331606215,\n",
       " 0.3247422680412371,\n",
       " 0.3230769230769231,\n",
       " 0.32142857142857145,\n",
       " 0.3197969543147208,\n",
       " 0.32323232323232326,\n",
       " 0.32160804020100503,\n",
       " 0.325,\n",
       " 0.3283582089552239,\n",
       " 0.32673267326732675,\n",
       " 0.3251231527093596,\n",
       " 0.3235294117647059,\n",
       " 0.32195121951219513,\n",
       " 0.32038834951456313,\n",
       " 0.3188405797101449,\n",
       " 0.3173076923076923,\n",
       " 0.32057416267942584,\n",
       " 0.319047619047619,\n",
       " 0.3222748815165877,\n",
       " 0.32075471698113206,\n",
       " 0.323943661971831,\n",
       " 0.32710280373831774,\n",
       " 0.32558139534883723,\n",
       " 0.3287037037037037,\n",
       " 0.3271889400921659,\n",
       " 0.3256880733944954,\n",
       " 0.3242009132420091,\n",
       " 0.32272727272727275,\n",
       " 0.3257918552036199,\n",
       " 0.32432432432432434,\n",
       " 0.32286995515695066,\n",
       " 0.32142857142857145,\n",
       " 0.32,\n",
       " 0.3185840707964602,\n",
       " 0.32158590308370044,\n",
       " 0.3201754385964912,\n",
       " 0.31877729257641924,\n",
       " 0.3173913043478261,\n",
       " 0.31601731601731603,\n",
       " 0.3146551724137931,\n",
       " 0.3133047210300429,\n",
       " 0.31196581196581197,\n",
       " 0.31063829787234043,\n",
       " 0.3093220338983051,\n",
       " 0.3080168776371308,\n",
       " 0.3067226890756303,\n",
       " 0.30962343096234307,\n",
       " 0.30833333333333335,\n",
       " 0.3112033195020747,\n",
       " 0.30991735537190085,\n",
       " 0.31275720164609055,\n",
       " 0.3114754098360656,\n",
       " 0.31020408163265306,\n",
       " 0.3089430894308943,\n",
       " 0.3076923076923077,\n",
       " 0.31048387096774194,\n",
       " 0.3092369477911647,\n",
       " 0.308,\n",
       " 0.30677290836653387,\n",
       " 0.3055555555555556,\n",
       " 0.30434782608695654,\n",
       " 0.30708661417322836,\n",
       " 0.30980392156862746,\n",
       " 0.30859375,\n",
       " 0.30739299610894943,\n",
       " 0.31007751937984496,\n",
       " 0.3088803088803089,\n",
       " 0.3076923076923077,\n",
       " 0.3065134099616858,\n",
       " 0.3053435114503817,\n",
       " 0.30798479087452474,\n",
       " 0.3106060606060606,\n",
       " 0.30943396226415093,\n",
       " 0.3082706766917293,\n",
       " 0.30711610486891383,\n",
       " 0.30970149253731344,\n",
       " 0.30855018587360594,\n",
       " 0.3111111111111111,\n",
       " 0.30996309963099633,\n",
       " 0.3125,\n",
       " 0.31135531135531136,\n",
       " 0.3102189781021898,\n",
       " 0.3090909090909091,\n",
       " 0.3079710144927536,\n",
       " 0.30685920577617326,\n",
       " 0.3057553956834532,\n",
       " 0.30824372759856633,\n",
       " 0.30714285714285716,\n",
       " 0.30604982206405695,\n",
       " 0.30851063829787234,\n",
       " 0.30742049469964666,\n",
       " 0.30985915492957744,\n",
       " 0.3087719298245614,\n",
       " 0.3076923076923077,\n",
       " 0.30662020905923343,\n",
       " 0.3055555555555556,\n",
       " 0.3079584775086505,\n",
       " 0.30689655172413793,\n",
       " 0.30927835051546393,\n",
       " 0.3082191780821918,\n",
       " 0.30716723549488056,\n",
       " 0.30612244897959184,\n",
       " 0.30847457627118646,\n",
       " 0.30743243243243246,\n",
       " 0.30976430976430974,\n",
       " 0.3087248322147651,\n",
       " 0.3110367892976589,\n",
       " 0.31,\n",
       " 0.3089700996677741,\n",
       " 0.31125827814569534,\n",
       " 0.31353135313531355,\n",
       " 0.3125,\n",
       " 0.31475409836065577,\n",
       " 0.3137254901960784,\n",
       " 0.31596091205211724,\n",
       " 0.3181818181818182,\n",
       " 0.32038834951456313,\n",
       " 0.3193548387096774,\n",
       " 0.3183279742765273,\n",
       " 0.32051282051282054,\n",
       " 0.3226837060702875,\n",
       " 0.321656050955414,\n",
       " 0.3238095238095238,\n",
       " 0.3227848101265823,\n",
       " 0.3217665615141956,\n",
       " 0.32075471698113206,\n",
       " 0.31974921630094044,\n",
       " 0.321875,\n",
       " 0.32087227414330216,\n",
       " 0.32298136645962733,\n",
       " 0.3219814241486068,\n",
       " 0.32098765432098764,\n",
       " 0.3230769230769231,\n",
       " 0.3220858895705521,\n",
       " 0.3241590214067278,\n",
       " 0.3231707317073171,\n",
       " 0.3221884498480243,\n",
       " 0.3212121212121212,\n",
       " 0.3202416918429003,\n",
       " 0.3192771084337349,\n",
       " 0.3213213213213213,\n",
       " 0.3203592814371258,\n",
       " 0.32238805970149254,\n",
       " 0.32142857142857145,\n",
       " 0.32047477744807124,\n",
       " 0.31952662721893493,\n",
       " 0.3185840707964602,\n",
       " 0.3176470588235294,\n",
       " 0.31671554252199413,\n",
       " 0.3157894736842105,\n",
       " 0.31486880466472306,\n",
       " 0.313953488372093,\n",
       " 0.3130434782608696,\n",
       " 0.31213872832369943,\n",
       " 0.31412103746397696,\n",
       " 0.3132183908045977,\n",
       " 0.3123209169054441,\n",
       " 0.31142857142857144,\n",
       " 0.31339031339031337,\n",
       " 0.3125,\n",
       " 0.311614730878187,\n",
       " 0.3107344632768362,\n",
       " 0.30985915492957744,\n",
       " 0.3089887640449438,\n",
       " 0.3081232492997199,\n",
       " 0.30726256983240224,\n",
       " 0.30919220055710306,\n",
       " 0.30833333333333335,\n",
       " 0.3074792243767313,\n",
       " 0.30662983425414364,\n",
       " 0.3085399449035813,\n",
       " 0.3076923076923077,\n",
       " 0.3095890410958904,\n",
       " 0.3087431693989071,\n",
       " 0.3079019073569482,\n",
       " 0.30978260869565216,\n",
       " 0.3089430894308943,\n",
       " 0.3108108108108108,\n",
       " 0.30997304582210244,\n",
       " 0.30913978494623656,\n",
       " 0.3109919571045576,\n",
       " 0.31283422459893045,\n",
       " 0.31466666666666665,\n",
       " 0.31382978723404253,\n",
       " 0.3129973474801061,\n",
       " 0.3148148148148148,\n",
       " 0.31398416886543534,\n",
       " 0.3131578947368421,\n",
       " 0.31496062992125984,\n",
       " 0.31413612565445026,\n",
       " 0.3133159268929504,\n",
       " 0.3125,\n",
       " 0.3142857142857143,\n",
       " 0.3134715025906736,\n",
       " 0.3152454780361757,\n",
       " 0.3170103092783505,\n",
       " 0.31876606683804626,\n",
       " 0.31794871794871793,\n",
       " 0.3171355498721228,\n",
       " 0.31887755102040816,\n",
       " 0.32061068702290074,\n",
       " 0.3197969543147208,\n",
       " 0.32151898734177214,\n",
       " 0.3207070707070707,\n",
       " 0.3198992443324937,\n",
       " 0.31909547738693467,\n",
       " 0.3182957393483709,\n",
       " 0.3175,\n",
       " 0.3167082294264339,\n",
       " 0.31840796019900497,\n",
       " 0.3200992555831266,\n",
       " 0.3193069306930693,\n",
       " 0.32098765432098764,\n",
       " 0.3226600985221675,\n",
       " 0.32432432432432434,\n",
       " 0.3235294117647059,\n",
       " 0.32273838630806845,\n",
       " 0.32195121951219513,\n",
       " 0.32116788321167883,\n",
       " 0.32038834951456313,\n",
       " 0.3196125907990315,\n",
       " 0.3188405797101449,\n",
       " 0.3180722891566265,\n",
       " 0.3173076923076923,\n",
       " 0.31654676258992803,\n",
       " 0.3181818181818182,\n",
       " 0.3198090692124105,\n",
       " 0.319047619047619,\n",
       " 0.3182897862232779,\n",
       " 0.3175355450236967,\n",
       " 0.31678486997635935,\n",
       " 0.3160377358490566,\n",
       " 0.31529411764705884,\n",
       " 0.3145539906103286,\n",
       " 0.31381733021077285,\n",
       " 0.3130841121495327,\n",
       " 0.3146853146853147,\n",
       " 0.313953488372093,\n",
       " 0.31554524361948955,\n",
       " 0.3148148148148148,\n",
       " 0.3163972286374134,\n",
       " 0.315668202764977,\n",
       " 0.31494252873563217,\n",
       " 0.31422018348623854,\n",
       " 0.3135011441647597,\n",
       " 0.3150684931506849,\n",
       " 0.3143507972665148,\n",
       " 0.3159090909090909,\n",
       " 0.31519274376417233,\n",
       " 0.3167420814479638,\n",
       " 0.3160270880361174,\n",
       " 0.3153153153153153,\n",
       " 0.3146067415730337,\n",
       " 0.31390134529147984,\n",
       " 0.3131991051454139,\n",
       " 0.3125,\n",
       " 0.31403118040089084,\n",
       " 0.31555555555555553,\n",
       " 0.3170731707317073,\n",
       " 0.3163716814159292,\n",
       " 0.31788079470198677,\n",
       " 0.31938325991189426,\n",
       " 0.31868131868131866,\n",
       " 0.31798245614035087,\n",
       " 0.3172866520787746,\n",
       " 0.3165938864628821,\n",
       " 0.31808278867102396,\n",
       " 0.31956521739130433,\n",
       " 0.3188720173535792,\n",
       " 0.3181818181818182,\n",
       " 0.3174946004319654,\n",
       " 0.3168103448275862,\n",
       " 0.3161290322580645,\n",
       " 0.315450643776824,\n",
       " 0.3169164882226981,\n",
       " 0.31837606837606836,\n",
       " 0.31769722814498935,\n",
       " 0.3191489361702128,\n",
       " 0.3184713375796178,\n",
       " 0.3199152542372881,\n",
       " 0.3192389006342495,\n",
       " 0.31856540084388185,\n",
       " 0.3178947368421053,\n",
       " 0.3172268907563025,\n",
       " 0.31865828092243187,\n",
       " 0.3179916317991632,\n",
       " 0.3173277661795407,\n",
       " 0.31875,\n",
       " 0.3180873180873181,\n",
       " 0.31742738589211617,\n",
       " 0.3167701863354037,\n",
       " 0.31611570247933884,\n",
       " 0.3154639175257732,\n",
       " 0.3168724279835391,\n",
       " 0.3162217659137577,\n",
       " 0.3155737704918033,\n",
       " 0.3149284253578732,\n",
       " 0.3142857142857143,\n",
       " 0.3136456211812627,\n",
       " 0.3130081300813008,\n",
       " 0.31237322515212984,\n",
       " 0.3117408906882591,\n",
       " 0.3111111111111111,\n",
       " 0.31048387096774194,\n",
       " 0.3118712273641851,\n",
       " 0.3112449799196787,\n",
       " 0.3106212424849699,\n",
       " 0.31,\n",
       " 0.3093812375249501,\n",
       " 0.30876494023904383,\n",
       " 0.3101391650099404,\n",
       " 0.30952380952380953,\n",
       " 0.3089108910891089,\n",
       " 0.3102766798418972,\n",
       " 0.3096646942800789,\n",
       " 0.3090551181102362,\n",
       " 0.3104125736738703,\n",
       " 0.30980392156862746,\n",
       " 0.30919765166340507,\n",
       " 0.30859375,\n",
       " 0.30994152046783624,\n",
       " 0.311284046692607,\n",
       " 0.3106796116504854,\n",
       " 0.31007751937984496,\n",
       " 0.30947775628626695,\n",
       " 0.3088803088803089,\n",
       " 0.31021194605009633,\n",
       " 0.3096153846153846,\n",
       " 0.30902111324376197,\n",
       " 0.30842911877394635,\n",
       " 0.3078393881453155,\n",
       " 0.30725190839694655,\n",
       " 0.30857142857142855,\n",
       " 0.30798479087452474,\n",
       " 0.30740037950664134,\n",
       " 0.3068181818181818,\n",
       " 0.30623818525519847,\n",
       " 0.30566037735849055,\n",
       " 0.3069679849340866,\n",
       " 0.3082706766917293,\n",
       " 0.30956848030018763,\n",
       " 0.3089887640449438,\n",
       " 0.3102803738317757,\n",
       " 0.30970149253731344,\n",
       " 0.3091247672253259,\n",
       " 0.30855018587360594,\n",
       " 0.3079777365491651,\n",
       " 0.3074074074074074,\n",
       " 0.3068391866913124,\n",
       " 0.3062730627306273,\n",
       " 0.30570902394106814,\n",
       " 0.30514705882352944,\n",
       " 0.30458715596330277,\n",
       " 0.304029304029304,\n",
       " 0.30347349177330896,\n",
       " 0.30474452554744524,\n",
       " 0.30418943533697634,\n",
       " 0.30363636363636365,\n",
       " 0.30490018148820325,\n",
       " 0.30434782608695654,\n",
       " 0.3037974683544304,\n",
       " 0.30324909747292417,\n",
       " 0.3027027027027027,\n",
       " 0.302158273381295,\n",
       " 0.3016157989228007,\n",
       " 0.3010752688172043,\n",
       " 0.3005366726296959,\n",
       " 0.3,\n",
       " 0.30124777183600715,\n",
       " 0.30071174377224197,\n",
       " 0.30017761989342806,\n",
       " 0.299645390070922,\n",
       " 0.2991150442477876,\n",
       " 0.29858657243816256,\n",
       " 0.2980599647266314,\n",
       " 0.2975352112676056,\n",
       " 0.29701230228471004,\n",
       " 0.2964912280701754,\n",
       " 0.29772329246935203,\n",
       " 0.29895104895104896,\n",
       " 0.29842931937172773,\n",
       " 0.2979094076655052,\n",
       " 0.29739130434782607,\n",
       " 0.296875,\n",
       " 0.29809358752166376,\n",
       " 0.2975778546712803,\n",
       " 0.2970639032815199,\n",
       " 0.296551724137931,\n",
       " 0.29604130808950085,\n",
       " 0.29553264604810997,\n",
       " 0.2967409948542024,\n",
       " 0.2962328767123288,\n",
       " 0.29743589743589743,\n",
       " 0.29692832764505117,\n",
       " 0.29642248722316866,\n",
       " 0.29591836734693877,\n",
       " 0.29541595925297115,\n",
       " 0.29491525423728815,\n",
       " 0.2961082910321489,\n",
       " 0.2972972972972973,\n",
       " 0.29679595278246207,\n",
       " 0.2962962962962963,\n",
       " 0.29747899159663865,\n",
       " 0.29697986577181207,\n",
       " 0.2964824120603015,\n",
       " 0.2976588628762542,\n",
       " 0.2988313856427379]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array6 = []\n",
    "array7 = []\n",
    "for i in range(len(array4) - 1):\n",
    "    array6.append(array4[i])\n",
    "    if i >= 59:\n",
    "        order = float(np.mean(array6))\n",
    "        array7.append(order)\n",
    "\n",
    "array7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25      , 0.26666667, 0.32222222, 0.30833333, 0.31      ,\n",
       "        0.30833333, 0.31904762, 0.31875   , 0.30740741],\n",
       "       [0.26229508, 0.26446281, 0.32596685, 0.31120332, 0.3089701 ,\n",
       "        0.30747922, 0.31828979, 0.31808732, 0.30683919],\n",
       "       [0.27419355, 0.26229508, 0.32417582, 0.30991736, 0.31125828,\n",
       "        0.30662983, 0.31753555, 0.31742739, 0.30627306],\n",
       "       [0.28571429, 0.2601626 , 0.32786885, 0.3127572 , 0.31353135,\n",
       "        0.30853994, 0.31678487, 0.31677019, 0.30570902],\n",
       "       [0.296875  , 0.25806452, 0.32608696, 0.31147541, 0.3125    ,\n",
       "        0.30769231, 0.31603774, 0.3161157 , 0.30514706],\n",
       "       [0.29230769, 0.264     , 0.32432432, 0.31020408, 0.3147541 ,\n",
       "        0.30958904, 0.31529412, 0.31546392, 0.30458716],\n",
       "       [0.28787879, 0.26190476, 0.32258065, 0.30894309, 0.31372549,\n",
       "        0.30874317, 0.31455399, 0.31687243, 0.3040293 ],\n",
       "       [0.28358209, 0.25984252, 0.32085561, 0.30769231, 0.31596091,\n",
       "        0.30790191, 0.31381733, 0.31622177, 0.30347349],\n",
       "       [0.27941176, 0.2578125 , 0.32446809, 0.31048387, 0.31818182,\n",
       "        0.30978261, 0.31308411, 0.31557377, 0.30474453],\n",
       "       [0.27536232, 0.25581395, 0.32275132, 0.30923695, 0.32038835,\n",
       "        0.30894309, 0.31468531, 0.31492843, 0.30418944],\n",
       "       [0.27142857, 0.25384615, 0.32105263, 0.308     , 0.31935484,\n",
       "        0.31081081, 0.31395349, 0.31428571, 0.30363636],\n",
       "       [0.26760563, 0.25954198, 0.32460733, 0.30677291, 0.31832797,\n",
       "        0.30997305, 0.31554524, 0.31364562, 0.30490018],\n",
       "       [0.26388889, 0.25757576, 0.32291667, 0.30555556, 0.32051282,\n",
       "        0.30913978, 0.31481481, 0.31300813, 0.30434783],\n",
       "       [0.26027397, 0.2556391 , 0.32124352, 0.30434783, 0.32268371,\n",
       "        0.31099196, 0.31639723, 0.31237323, 0.30379747],\n",
       "       [0.27027027, 0.26119403, 0.32474227, 0.30708661, 0.32165605,\n",
       "        0.31283422, 0.3156682 , 0.31174089, 0.3032491 ],\n",
       "       [0.26666667, 0.25925926, 0.32307692, 0.30980392, 0.32380952,\n",
       "        0.31466667, 0.31494253, 0.31111111, 0.3027027 ],\n",
       "       [0.26315789, 0.25735294, 0.32142857, 0.30859375, 0.32278481,\n",
       "        0.31382979, 0.31422018, 0.31048387, 0.30215827],\n",
       "       [0.25974026, 0.26277372, 0.31979695, 0.307393  , 0.32176656,\n",
       "        0.31299735, 0.31350114, 0.31187123, 0.3016158 ],\n",
       "       [0.25641026, 0.26086957, 0.32323232, 0.31007752, 0.32075472,\n",
       "        0.31481481, 0.31506849, 0.31124498, 0.30107527],\n",
       "       [0.25316456, 0.25899281, 0.32160804, 0.30888031, 0.31974922,\n",
       "        0.31398417, 0.3143508 , 0.31062124, 0.30053667],\n",
       "       [0.25      , 0.25714286, 0.325     , 0.30769231, 0.321875  ,\n",
       "        0.31315789, 0.31590909, 0.31      , 0.3       ],\n",
       "       [0.24691358, 0.25531915, 0.32835821, 0.30651341, 0.32087227,\n",
       "        0.31496063, 0.31519274, 0.30938124, 0.30124777],\n",
       "       [0.24390244, 0.25352113, 0.32673267, 0.30534351, 0.32298137,\n",
       "        0.31413613, 0.31674208, 0.30876494, 0.30071174],\n",
       "       [0.25301205, 0.25174825, 0.32512315, 0.30798479, 0.32198142,\n",
       "        0.31331593, 0.31602709, 0.31013917, 0.30017762],\n",
       "       [0.25      , 0.25      , 0.32352941, 0.31060606, 0.32098765,\n",
       "        0.3125    , 0.31531532, 0.30952381, 0.29964539],\n",
       "       [0.24705882, 0.25517241, 0.32195122, 0.30943396, 0.32307692,\n",
       "        0.31428571, 0.31460674, 0.30891089, 0.29911504],\n",
       "       [0.25581395, 0.26027397, 0.32038835, 0.30827068, 0.32208589,\n",
       "        0.3134715 , 0.31390135, 0.31027668, 0.29858657],\n",
       "       [0.25287356, 0.26530612, 0.31884058, 0.3071161 , 0.32415902,\n",
       "        0.31524548, 0.31319911, 0.30966469, 0.29805996],\n",
       "       [0.25      , 0.27027027, 0.31730769, 0.30970149, 0.32317073,\n",
       "        0.31701031, 0.3125    , 0.30905512, 0.29753521],\n",
       "       [0.24719101, 0.27516779, 0.32057416, 0.30855019, 0.32218845,\n",
       "        0.31876607, 0.31403118, 0.31041257, 0.2970123 ],\n",
       "       [0.25555556, 0.28      , 0.31904762, 0.31111111, 0.32121212,\n",
       "        0.31794872, 0.31555556, 0.30980392, 0.29649123],\n",
       "       [0.25274725, 0.28476821, 0.32227488, 0.3099631 , 0.32024169,\n",
       "        0.31713555, 0.31707317, 0.30919765, 0.29772329],\n",
       "       [0.25      , 0.28289474, 0.32075472, 0.3125    , 0.31927711,\n",
       "        0.31887755, 0.31637168, 0.30859375, 0.29895105],\n",
       "       [0.24731183, 0.2875817 , 0.32394366, 0.31135531, 0.32132132,\n",
       "        0.32061069, 0.31788079, 0.30994152, 0.29842932],\n",
       "       [0.24468085, 0.29220779, 0.3271028 , 0.31021898, 0.32035928,\n",
       "        0.31979695, 0.31938326, 0.31128405, 0.29790941],\n",
       "       [0.25263158, 0.29032258, 0.3255814 , 0.30909091, 0.32238806,\n",
       "        0.32151899, 0.31868132, 0.31067961, 0.2973913 ],\n",
       "       [0.25      , 0.28846154, 0.3287037 , 0.30797101, 0.32142857,\n",
       "        0.32070707, 0.31798246, 0.31007752, 0.296875  ],\n",
       "       [0.25773196, 0.29299363, 0.32718894, 0.30685921, 0.32047478,\n",
       "        0.31989924, 0.31728665, 0.30947776, 0.29809359],\n",
       "       [0.25510204, 0.29113924, 0.32568807, 0.3057554 , 0.31952663,\n",
       "        0.31909548, 0.31659389, 0.30888031, 0.29757785],\n",
       "       [0.25252525, 0.28930818, 0.32420091, 0.30824373, 0.31858407,\n",
       "        0.31829574, 0.31808279, 0.31021195, 0.2970639 ],\n",
       "       [0.25      , 0.29375   , 0.32272727, 0.30714286, 0.31764706,\n",
       "        0.3175    , 0.31956522, 0.30961538, 0.29655172],\n",
       "       [0.24752475, 0.29192547, 0.32579186, 0.30604982, 0.31671554,\n",
       "        0.31670823, 0.31887202, 0.30902111, 0.29604131],\n",
       "       [0.24509804, 0.29012346, 0.32432432, 0.30851064, 0.31578947,\n",
       "        0.31840796, 0.31818182, 0.30842912, 0.29553265],\n",
       "       [0.25242718, 0.28834356, 0.32286996, 0.30742049, 0.3148688 ,\n",
       "        0.32009926, 0.3174946 , 0.30783939, 0.29674099],\n",
       "       [0.25      , 0.28658537, 0.32142857, 0.30985915, 0.31395349,\n",
       "        0.31930693, 0.31681034, 0.30725191, 0.29623288],\n",
       "       [0.24761905, 0.28484848, 0.32      , 0.30877193, 0.31304348,\n",
       "        0.32098765, 0.31612903, 0.30857143, 0.2974359 ],\n",
       "       [0.24528302, 0.28313253, 0.31858407, 0.30769231, 0.31213873,\n",
       "        0.3226601 , 0.31545064, 0.30798479, 0.29692833],\n",
       "       [0.25233645, 0.28742515, 0.3215859 , 0.30662021, 0.31412104,\n",
       "        0.32432432, 0.31691649, 0.30740038, 0.29642249],\n",
       "       [0.25925926, 0.29166667, 0.32017544, 0.30555556, 0.31321839,\n",
       "        0.32352941, 0.31837607, 0.30681818, 0.29591837],\n",
       "       [0.26605505, 0.29585799, 0.31877729, 0.30795848, 0.31232092,\n",
       "        0.32273839, 0.31769723, 0.30623819, 0.29541596],\n",
       "       [0.26363636, 0.3       , 0.3173913 , 0.30689655, 0.31142857,\n",
       "        0.32195122, 0.31914894, 0.30566038, 0.29491525],\n",
       "       [0.26126126, 0.30409357, 0.31601732, 0.30927835, 0.31339031,\n",
       "        0.32116788, 0.31847134, 0.30696798, 0.29610829],\n",
       "       [0.25892857, 0.30813953, 0.31465517, 0.30821918, 0.3125    ,\n",
       "        0.32038835, 0.31991525, 0.30827068, 0.2972973 ],\n",
       "       [0.26548673, 0.30635838, 0.31330472, 0.30716724, 0.31161473,\n",
       "        0.31961259, 0.3192389 , 0.30956848, 0.29679595],\n",
       "       [0.26315789, 0.31034483, 0.31196581, 0.30612245, 0.31073446,\n",
       "        0.31884058, 0.3185654 , 0.30898876, 0.2962963 ],\n",
       "       [0.26086957, 0.31428571, 0.3106383 , 0.30847458, 0.30985915,\n",
       "        0.31807229, 0.31789474, 0.31028037, 0.29747899],\n",
       "       [0.26724138, 0.31818182, 0.30932203, 0.30743243, 0.30898876,\n",
       "        0.31730769, 0.31722689, 0.30970149, 0.29697987],\n",
       "       [0.26495726, 0.3220339 , 0.30801688, 0.30976431, 0.30812325,\n",
       "        0.31654676, 0.31865828, 0.30912477, 0.29648241],\n",
       "       [0.27118644, 0.32022472, 0.30672269, 0.30872483, 0.30726257,\n",
       "        0.31818182, 0.31799163, 0.30855019, 0.29765886],\n",
       "       [0.26890756, 0.31843575, 0.30962343, 0.31103679, 0.3091922 ,\n",
       "        0.31980907, 0.31732777, 0.30797774, 0.29883139]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matriz5 = matriz(60, array7)\n",
    "matriz5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60, 10), (60, 10), (60, 9), (60, 9))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matriz2.shape, matriz3.shape, matriz4.shape, matriz5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrizfloat = matriz2[:,1:]\n",
    "matrizint = matriz3[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60, 9), (60, 9), (60, 9), (60, 9))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrizfloat.shape, matrizint.shape, matriz4.shape, matriz5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1672, 600, 600, 540, 540)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(array2), len(array3), len(array4), len(array5), len(array7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = matrizfloat[:,:(matrizfloat.shape[1] - 1)]\n",
    "x2 = matriz4[:,:(matriz4.shape[1] - 1)]\n",
    "x3 = matriz5[:,:(matriz5.shape[1] - 1)]\n",
    "\n",
    "y = matrizint[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 24)\n"
     ]
    }
   ],
   "source": [
    "# Empilhar as matrizes para ter um eixo extra (60, 8, 3)\n",
    "X_stack = np.stack([x1, x2, x3], axis=2)  # Formato (60, 8, 3)\n",
    "\n",
    "# Reorganizar para intercalar coluna por coluna\n",
    "X_intercalado = X_stack.reshape(60, -1)  # Agora está no formato (60, 24)\n",
    "\n",
    "print(X_intercalado.shape)  # Saída: (60, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia do modelo: 0.5000\n",
      "F1-Score do modelo: 0.4444\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "X = X_intercalado\n",
    "\n",
    "# Dividir os dados em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Criar e treinar o modelo XGBoost\n",
    "model = xgb.XGBClassifier(\n",
    "    objective='multi:softmax',  # Classificação multiclasse\n",
    "    num_class=2,  # Número de categorias na saída\n",
    "    eval_metric='mlogloss',\n",
    "    learning_rate=0.01,\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Fazer previsões\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Avaliar o modelo\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f'Acurácia do modelo: {accuracy:.4f}')\n",
    "print(f'F1-Score do modelo: {f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-02 12:38:14.693946: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-02 12:38:14.877703: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-02 12:38:14.991632: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740933495.144493   24559 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740933495.182876   24559 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-02 12:38:15.557656: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/darkcover/Documentos/Out/venv/lib/python3.12/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "/home/darkcover/Documentos/Out/venv/lib/python3.12/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.13.0 and strictly below 2.16.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.18.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "#activation = tf.keras.activations.elu\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "\n",
    "import skfuzzy as fuzz\n",
    "\n",
    "# Libs\n",
    "import time\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reden(array1, array3, m, n):\n",
    "    print(m, n)\n",
    "\n",
    "    # Dividindo os dados em treino e teste\n",
    "    X = np.array(array3).astype(\"float32\")  # Certificar que X está no formato correto\n",
    "    y = np.array(array1)\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Ajustando dimensões corretamente\n",
    "    print(\"Shape inicial de x_train:\", x_train.shape)  # Deve ser (48, 24, 1)\n",
    "\n",
    "    input_shape = (n, 1)  # Usando n diretamente\n",
    "\n",
    "    # Verificar os valores de y_train antes da conversão\n",
    "    print(\"Valores únicos em y_train antes da conversão:\", np.unique(y_train))\n",
    "\n",
    "    # Se necessário, garantir que y_train só tenha 0 e 1\n",
    "    y_train = np.where(y_train > 0, 1, 0)\n",
    "    y_test = np.where(y_test > 0, 1, 0)\n",
    "\n",
    "    # Converter para categórico\n",
    "    num_classes = 2\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "    print(\"Shape de y_train após conversão:\", y_train.shape)  # Deve ser (48, 2)\n",
    "    print(\"Shape de x_train após conversão:\", x_train.shape)  # Deve ser (48, 24, 1)\n",
    "\n",
    "    # Definição do modelo\n",
    "    model = keras.Sequential([\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation=\"relu\", use_bias=True),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(64, activation=\"relu\", use_bias=True),\n",
    "        layers.Dense(32, activation=tf.keras.activations.swish, use_bias=True),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        loss=tfa.losses.SigmoidFocalCrossEntropy(alpha=0.25, gamma=2.0),\n",
    "        optimizer=tf.keras.optimizers.AdamW(learning_rate=0.001, weight_decay=1e-4),\n",
    "        metrics=['accuracy', Precision(name=\"precision\"), Recall(name=\"recall\")]\n",
    "    )\n",
    "\n",
    "    # Treinamento\n",
    "    batch_size = 2**10\n",
    "    epochs = 50\n",
    "    model.fit(\n",
    "        x_train, y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_split=0.2\n",
    "    )\n",
    "\n",
    "    # Avaliação\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print(f\"Test loss: {score[0]:.4f}\")\n",
    "    print(f\"Test accuracy: {score[1]:.4f}\")\n",
    "    print(f\"Precision: {score[2]:.4f}\")\n",
    "    print(f\"Recall: {score[3]:.4f}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 24)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 24\n",
      "Shape inicial de x_train: (48, 24)\n",
      "Valores únicos em y_train antes da conversão: [0 1]\n",
      "Shape de y_train após conversão: (48, 2)\n",
      "Shape de x_train após conversão: (48, 24)\n",
      "Epoch 1/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - accuracy: 0.8684 - loss: 0.9758 - precision: 0.8684 - recall: 0.8684 - val_accuracy: 0.6000 - val_loss: 1.5366 - val_precision: 0.6000 - val_recall: 0.6000\n",
      "Epoch 2/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.8421 - loss: 1.1782 - precision: 0.8421 - recall: 0.8421 - val_accuracy: 0.6000 - val_loss: 0.6279 - val_precision: 0.6000 - val_recall: 0.6000\n",
      "Epoch 3/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.7895 - loss: 1.1674 - precision: 0.7895 - recall: 0.7895 - val_accuracy: 0.7000 - val_loss: 0.1801 - val_precision: 0.7000 - val_recall: 0.7000\n",
      "Epoch 4/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.6316 - loss: 1.0721 - precision: 0.6316 - recall: 0.6316 - val_accuracy: 0.3000 - val_loss: 0.2713 - val_precision: 0.3000 - val_recall: 0.3000\n",
      "Epoch 5/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.5789 - loss: 0.6631 - precision: 0.5789 - recall: 0.5789 - val_accuracy: 0.4000 - val_loss: 0.3196 - val_precision: 0.4000 - val_recall: 0.4000\n",
      "Epoch 6/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.6316 - loss: 0.9773 - precision: 0.6316 - recall: 0.6316 - val_accuracy: 0.5000 - val_loss: 0.2963 - val_precision: 0.5000 - val_recall: 0.5000\n",
      "Epoch 7/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.5789 - loss: 0.6510 - precision: 0.5789 - recall: 0.5789 - val_accuracy: 0.6000 - val_loss: 0.2150 - val_precision: 0.6000 - val_recall: 0.6000\n",
      "Epoch 8/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.7632 - loss: 0.5641 - precision: 0.7632 - recall: 0.7632 - val_accuracy: 0.6000 - val_loss: 0.2119 - val_precision: 0.6000 - val_recall: 0.6000\n",
      "Epoch 9/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.8158 - loss: 0.8105 - precision: 0.8158 - recall: 0.8158 - val_accuracy: 0.5000 - val_loss: 0.2618 - val_precision: 0.5000 - val_recall: 0.5000\n",
      "Epoch 10/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.8158 - loss: 0.3802 - precision: 0.8158 - recall: 0.8158 - val_accuracy: 0.5000 - val_loss: 0.3237 - val_precision: 0.5000 - val_recall: 0.5000\n",
      "Epoch 11/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.8947 - loss: 0.3701 - precision: 0.8947 - recall: 0.8947 - val_accuracy: 0.6000 - val_loss: 0.3599 - val_precision: 0.6000 - val_recall: 0.6000\n",
      "Epoch 12/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.7632 - loss: 0.3748 - precision: 0.7632 - recall: 0.7632 - val_accuracy: 0.6000 - val_loss: 0.3824 - val_precision: 0.6000 - val_recall: 0.6000\n",
      "Epoch 13/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.7632 - loss: 0.4366 - precision: 0.7632 - recall: 0.7632 - val_accuracy: 0.6000 - val_loss: 0.3838 - val_precision: 0.6000 - val_recall: 0.6000\n",
      "Epoch 14/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.8947 - loss: 0.2992 - precision: 0.8947 - recall: 0.8947 - val_accuracy: 0.5000 - val_loss: 0.3503 - val_precision: 0.5000 - val_recall: 0.5000\n",
      "Epoch 15/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.8421 - loss: 0.1535 - precision: 0.8421 - recall: 0.8421 - val_accuracy: 0.5000 - val_loss: 0.3124 - val_precision: 0.5000 - val_recall: 0.5000\n",
      "Epoch 16/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.7632 - loss: 0.4323 - precision: 0.7632 - recall: 0.7632 - val_accuracy: 0.5000 - val_loss: 0.2681 - val_precision: 0.5000 - val_recall: 0.5000\n",
      "Epoch 17/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.7105 - loss: 0.4474 - precision: 0.7105 - recall: 0.7105 - val_accuracy: 0.4000 - val_loss: 0.2340 - val_precision: 0.4000 - val_recall: 0.4000\n",
      "Epoch 18/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.8684 - loss: 0.2197 - precision: 0.8684 - recall: 0.8684 - val_accuracy: 0.4000 - val_loss: 0.2203 - val_precision: 0.4000 - val_recall: 0.4000\n",
      "Epoch 19/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.8421 - loss: 0.2717 - precision: 0.8421 - recall: 0.8421 - val_accuracy: 0.5000 - val_loss: 0.2212 - val_precision: 0.5000 - val_recall: 0.5000\n",
      "Epoch 20/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.8421 - loss: 0.1402 - precision: 0.8421 - recall: 0.8421 - val_accuracy: 0.5000 - val_loss: 0.2308 - val_precision: 0.5000 - val_recall: 0.5000\n",
      "Epoch 21/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.6842 - loss: 0.2427 - precision: 0.6842 - recall: 0.6842 - val_accuracy: 0.6000 - val_loss: 0.2421 - val_precision: 0.6000 - val_recall: 0.6000\n",
      "Epoch 22/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step - accuracy: 0.7895 - loss: 0.3221 - precision: 0.7895 - recall: 0.7895 - val_accuracy: 0.6000 - val_loss: 0.2475 - val_precision: 0.6000 - val_recall: 0.6000\n",
      "Epoch 23/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.8158 - loss: 0.1843 - precision: 0.8158 - recall: 0.8158 - val_accuracy: 0.6000 - val_loss: 0.2425 - val_precision: 0.6000 - val_recall: 0.6000\n",
      "Epoch 24/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.6842 - loss: 0.3558 - precision: 0.6842 - recall: 0.6842 - val_accuracy: 0.6000 - val_loss: 0.2239 - val_precision: 0.6000 - val_recall: 0.6000\n",
      "Epoch 25/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.6316 - loss: 0.2586 - precision: 0.6316 - recall: 0.6316 - val_accuracy: 0.6000 - val_loss: 0.1988 - val_precision: 0.6000 - val_recall: 0.6000\n",
      "Epoch 26/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.7895 - loss: 0.1816 - precision: 0.7895 - recall: 0.7895 - val_accuracy: 0.5000 - val_loss: 0.1853 - val_precision: 0.5000 - val_recall: 0.5000\n",
      "Epoch 27/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8158 - loss: 0.1668 - precision: 0.8158 - recall: 0.8158 - val_accuracy: 0.5000 - val_loss: 0.1865 - val_precision: 0.5000 - val_recall: 0.5000\n",
      "Epoch 28/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.7895 - loss: 0.1876 - precision: 0.7895 - recall: 0.7895 - val_accuracy: 0.4000 - val_loss: 0.2067 - val_precision: 0.4000 - val_recall: 0.4000\n",
      "Epoch 29/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.8684 - loss: 0.3042 - precision: 0.8684 - recall: 0.8684 - val_accuracy: 0.5000 - val_loss: 0.2275 - val_precision: 0.5000 - val_recall: 0.5000\n",
      "Epoch 30/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.8684 - loss: 0.3129 - precision: 0.8684 - recall: 0.8684 - val_accuracy: 0.5000 - val_loss: 0.2368 - val_precision: 0.5000 - val_recall: 0.5000\n",
      "Epoch 31/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.8684 - loss: 0.2290 - precision: 0.8684 - recall: 0.8684 - val_accuracy: 0.5000 - val_loss: 0.2270 - val_precision: 0.5000 - val_recall: 0.5000\n",
      "Epoch 32/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.8421 - loss: 0.3663 - precision: 0.8421 - recall: 0.8421 - val_accuracy: 0.5000 - val_loss: 0.2159 - val_precision: 0.5000 - val_recall: 0.5000\n",
      "Epoch 33/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.8684 - loss: 0.3159 - precision: 0.8684 - recall: 0.8684 - val_accuracy: 0.4000 - val_loss: 0.1957 - val_precision: 0.4000 - val_recall: 0.4000\n",
      "Epoch 34/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.7895 - loss: 0.2135 - precision: 0.7895 - recall: 0.7895 - val_accuracy: 0.4000 - val_loss: 0.1840 - val_precision: 0.4000 - val_recall: 0.4000\n",
      "Epoch 35/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.8684 - loss: 0.1710 - precision: 0.8684 - recall: 0.8684 - val_accuracy: 0.4000 - val_loss: 0.1837 - val_precision: 0.4000 - val_recall: 0.4000\n",
      "Epoch 36/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.8158 - loss: 0.1571 - precision: 0.8158 - recall: 0.8158 - val_accuracy: 0.8000 - val_loss: 0.1921 - val_precision: 0.8000 - val_recall: 0.8000\n",
      "Epoch 37/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.8684 - loss: 0.2534 - precision: 0.8684 - recall: 0.8684 - val_accuracy: 0.7000 - val_loss: 0.1973 - val_precision: 0.7000 - val_recall: 0.7000\n",
      "Epoch 38/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.6316 - loss: 0.2976 - precision: 0.6316 - recall: 0.6316 - val_accuracy: 0.7000 - val_loss: 0.1969 - val_precision: 0.7000 - val_recall: 0.7000\n",
      "Epoch 39/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.8158 - loss: 0.1997 - precision: 0.8158 - recall: 0.8158 - val_accuracy: 0.6000 - val_loss: 0.1927 - val_precision: 0.6000 - val_recall: 0.6000\n",
      "Epoch 40/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.8158 - loss: 0.1635 - precision: 0.8158 - recall: 0.8158 - val_accuracy: 0.7000 - val_loss: 0.1866 - val_precision: 0.7000 - val_recall: 0.7000\n",
      "Epoch 41/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.8684 - loss: 0.1144 - precision: 0.8684 - recall: 0.8684 - val_accuracy: 0.5000 - val_loss: 0.1822 - val_precision: 0.5000 - val_recall: 0.5000\n",
      "Epoch 42/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.7368 - loss: 0.3052 - precision: 0.7368 - recall: 0.7368 - val_accuracy: 0.5000 - val_loss: 0.1807 - val_precision: 0.5000 - val_recall: 0.5000\n",
      "Epoch 43/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.8158 - loss: 0.1804 - precision: 0.8158 - recall: 0.8158 - val_accuracy: 0.4000 - val_loss: 0.1858 - val_precision: 0.4000 - val_recall: 0.4000\n",
      "Epoch 44/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.7368 - loss: 0.1784 - precision: 0.7368 - recall: 0.7368 - val_accuracy: 0.5000 - val_loss: 0.2061 - val_precision: 0.5000 - val_recall: 0.5000\n",
      "Epoch 45/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.8158 - loss: 0.0791 - precision: 0.8158 - recall: 0.8158 - val_accuracy: 0.6000 - val_loss: 0.2500 - val_precision: 0.6000 - val_recall: 0.6000\n",
      "Epoch 46/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.8684 - loss: 0.1770 - precision: 0.8684 - recall: 0.8684 - val_accuracy: 0.6000 - val_loss: 0.2919 - val_precision: 0.6000 - val_recall: 0.6000\n",
      "Epoch 47/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.8684 - loss: 0.1069 - precision: 0.8684 - recall: 0.8684 - val_accuracy: 0.6000 - val_loss: 0.3275 - val_precision: 0.6000 - val_recall: 0.6000\n",
      "Epoch 48/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.7632 - loss: 0.3018 - precision: 0.7632 - recall: 0.7632 - val_accuracy: 0.6000 - val_loss: 0.3491 - val_precision: 0.6000 - val_recall: 0.6000\n",
      "Epoch 49/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.8421 - loss: 0.1619 - precision: 0.8421 - recall: 0.8421 - val_accuracy: 0.6000 - val_loss: 0.3509 - val_precision: 0.6000 - val_recall: 0.6000\n",
      "Epoch 50/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.8421 - loss: 0.2703 - precision: 0.8421 - recall: 0.8421 - val_accuracy: 0.6000 - val_loss: 0.3143 - val_precision: 0.6000 - val_recall: 0.6000\n",
      "Test loss: 0.1995\n",
      "Test accuracy: 0.6667\n",
      "Precision: 0.6667\n",
      "Recall: 0.6667\n"
     ]
    }
   ],
   "source": [
    "X = X_intercalado\n",
    "model = reden(y, X, X.shape[0], X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
