{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kardec data read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matriz(num_colunas, array1):\n",
    "    \"\"\"\n",
    "    Gera uma matriz sequencial a partir de um array, com o número de colunas especificado.\n",
    "\n",
    "    Args:\n",
    "        array (list ou np.ndarray): Array de entrada.\n",
    "        num_colunas (int): Número de colunas desejado na matriz.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Matriz sequencial.\n",
    "    \"\"\"\n",
    "    if num_colunas > len(array1):\n",
    "        raise ValueError(\"O número de colunas não pode ser maior que o tamanho do array.\")\n",
    "\n",
    "    # Número de linhas na matriz\n",
    "    num_linhas = len(array1) - num_colunas + 1\n",
    "\n",
    "    # Criando a matriz sequencial\n",
    "    matriz = np.array([array1[i:i + num_colunas] for i in range(num_linhas)])\n",
    "    return matriz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
       "       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
       "       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
       "       221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
       "       234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
       "       247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n",
       "       260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n",
       "       273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n",
       "       286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n",
       "       299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
       "       312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324,\n",
       "       325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337,\n",
       "       338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350,\n",
       "       351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
       "       364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376,\n",
       "       377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389,\n",
       "       390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402,\n",
       "       403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415,\n",
       "       416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428,\n",
       "       429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441,\n",
       "       442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454,\n",
       "       455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467,\n",
       "       468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480,\n",
       "       481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493,\n",
       "       494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506,\n",
       "       507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519,\n",
       "       520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532,\n",
       "       533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545,\n",
       "       546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558,\n",
       "       559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571,\n",
       "       572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584,\n",
       "       585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597,\n",
       "       598, 599])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array1 = np.arange(600)\n",
    "array1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   1,   2, ...,  57,  58,  59],\n",
       "       [  1,   2,   3, ...,  58,  59,  60],\n",
       "       [  2,   3,   4, ...,  59,  60,  61],\n",
       "       ...,\n",
       "       [538, 539, 540, ..., 595, 596, 597],\n",
       "       [539, 540, 541, ..., 596, 597, 598],\n",
       "       [540, 541, 542, ..., 597, 598, 599]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matriz1 = matriz(60, array1)\n",
    "matriz1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['n', 'Entrada', 'Odd', 'P60', 'P120', 'P180', 'P240', 'P300', 'P360',\n",
       "       'P420', 'P480', 'P540', 'P600', 'P660', 'P720', 'P780', 'P840', 'P900',\n",
       "       'P960', 'P1020', 'P1080', 'P1140', 'P1200', 'P1260', 'P1320', 'P1380',\n",
       "       'P1440', 'P1500', 'P1560', 'P1620', 'P1680', 'P1740', 'P1800', 'P1860',\n",
       "       'P1920', 'P1980', 'P1200.1', 'Media Movel', 'Unnamed: 38',\n",
       "       'Unnamed: 39', 'Unnamed: 40', 'Unnamed: 41', 'Unnamed: 42',\n",
       "       'Unnamed: 43', 'Unnamed: 44', 'Unnamed: 45', 'Acertos 60',\n",
       "       'Unnamed: 47', 'Unnamed: 48', 'Unnamed: 49', 'Unnamed: 50',\n",
       "       'Unnamed: 51', 'Unnamed: 52', 'Unnamed: 53', 'Unnamed: 54',\n",
       "       'Unnamed: 55', 'Unnamed: 56', 'Unnamed: 57', 'Unnamed: 58',\n",
       "       'Unnamed: 59', 'Unnamed: 60', 'Unnamed: 61', 'Unnamed: 62',\n",
       "       'Unnamed: 63', 'Unnamed: 64', 'Unnamed: 65', 'Unnamed: 66',\n",
       "       'Unnamed: 67', 'Unnamed: 68', 'Acertos Geral', 'Média Global',\n",
       "       'Unnamed: 71', 'Unnamed: 72', 'Unnamed: 73', 'Unnamed: 74',\n",
       "       'Unnamed: 75', 'Unnamed: 76', 'Unnamed: 77', 'Unnamed: 78',\n",
       "       'Unnamed: 79', 'Unnamed: 80', 'Unnamed: 81', 'Unnamed: 82',\n",
       "       'Unnamed: 83', 'acertos_intervalos', 'Unnamed: 85'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = pd.read_csv('/home/darkcover/Documentos/Out/dados/Saidas/FUNCOES/DOUBLE - 17_09_s1.csv')\n",
    "data1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        11,6\n",
       "1        2,02\n",
       "2        2,02\n",
       "3        1,54\n",
       "4         2,2\n",
       "        ...  \n",
       "1667        1\n",
       "1668     4,34\n",
       "1669    19,98\n",
       "1670     2,52\n",
       "1671     10,2\n",
       "Name: Entrada, Length: 1672, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array2 = data1['Entrada']\n",
    "array2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6.0,\n",
       " 2.02,\n",
       " 2.02,\n",
       " 1.54,\n",
       " 2.2,\n",
       " 1.51,\n",
       " 6.0,\n",
       " 3.89,\n",
       " 1.02,\n",
       " 1.48,\n",
       " 1.25,\n",
       " 1.07,\n",
       " 1.93,\n",
       " 4.33,\n",
       " 3.59,\n",
       " 3.26,\n",
       " 1.83,\n",
       " 1.6,\n",
       " 2.07,\n",
       " 6.0,\n",
       " 1.21,\n",
       " 5.46,\n",
       " 3.81,\n",
       " 1.3,\n",
       " 1.95,\n",
       " 1.04,\n",
       " 1.28,\n",
       " 1.03,\n",
       " 1.05,\n",
       " 3.39,\n",
       " 1.05,\n",
       " 2.59,\n",
       " 2.53,\n",
       " 6.0,\n",
       " 1.34,\n",
       " 1.56,\n",
       " 1.0,\n",
       " 1.22,\n",
       " 1.05,\n",
       " 6.0,\n",
       " 2.54,\n",
       " 2.44,\n",
       " 2.34,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 1.44,\n",
       " 1.63,\n",
       " 2.57,\n",
       " 1.73,\n",
       " 1.22,\n",
       " 1.34,\n",
       " 1.33,\n",
       " 1.0,\n",
       " 2.87,\n",
       " 2.28,\n",
       " 6.0,\n",
       " 1.0,\n",
       " 1.73,\n",
       " 1.05,\n",
       " 1.52,\n",
       " 5.63,\n",
       " 4.44,\n",
       " 3.91,\n",
       " 6.0,\n",
       " 2.44,\n",
       " 1.43,\n",
       " 1.83,\n",
       " 1.31,\n",
       " 1.61,\n",
       " 1.73,\n",
       " 1.4,\n",
       " 2.12,\n",
       " 1.49,\n",
       " 5.97,\n",
       " 1.12,\n",
       " 1.55,\n",
       " 1.4,\n",
       " 2.62,\n",
       " 2.87,\n",
       " 1.77,\n",
       " 1.02,\n",
       " 1.28,\n",
       " 6.0,\n",
       " 2.18,\n",
       " 1.99,\n",
       " 4.71,\n",
       " 1.01,\n",
       " 1.15,\n",
       " 1.41,\n",
       " 6.0,\n",
       " 1.92,\n",
       " 1.2,\n",
       " 1.0,\n",
       " 2.35,\n",
       " 6.0,\n",
       " 1.24,\n",
       " 4.81,\n",
       " 1.0,\n",
       " 1.27,\n",
       " 1.49,\n",
       " 1.94,\n",
       " 1.98,\n",
       " 5.93,\n",
       " 1.76,\n",
       " 1.9,\n",
       " 1.71,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 2.03,\n",
       " 1.14,\n",
       " 1.29,\n",
       " 6.0,\n",
       " 1.0,\n",
       " 2.21,\n",
       " 3.09,\n",
       " 1.49,\n",
       " 3.34,\n",
       " 1.04,\n",
       " 2.26,\n",
       " 1.21,\n",
       " 1.26,\n",
       " 1.01,\n",
       " 1.21,\n",
       " 3.08,\n",
       " 2.7,\n",
       " 2.3,\n",
       " 2.51,\n",
       " 1.17,\n",
       " 1.02,\n",
       " 4.1,\n",
       " 1.17,\n",
       " 1.61,\n",
       " 6.0,\n",
       " 2.74,\n",
       " 1.67,\n",
       " 5.5,\n",
       " 1.41,\n",
       " 1.13,\n",
       " 1.0,\n",
       " 1.49,\n",
       " 2.3,\n",
       " 1.31,\n",
       " 1.18,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 3.16,\n",
       " 4.08,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 2.16,\n",
       " 5.1,\n",
       " 6.0,\n",
       " 1.65,\n",
       " 1.0,\n",
       " 6.0,\n",
       " 2.12,\n",
       " 1.1,\n",
       " 6.0,\n",
       " 1.13,\n",
       " 1.64,\n",
       " 1.01,\n",
       " 2.11,\n",
       " 1.21,\n",
       " 1.71,\n",
       " 3.63,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 3.42,\n",
       " 6.0,\n",
       " 5.45,\n",
       " 2.62,\n",
       " 3.2,\n",
       " 6.0,\n",
       " 3.1,\n",
       " 3.65,\n",
       " 2.88,\n",
       " 1.74,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 2.47,\n",
       " 6.0,\n",
       " 1.52,\n",
       " 1.5,\n",
       " 2.36,\n",
       " 1.0,\n",
       " 6.0,\n",
       " 1.87,\n",
       " 2.11,\n",
       " 5.53,\n",
       " 1.46,\n",
       " 1.14,\n",
       " 6.0,\n",
       " 1.0,\n",
       " 2.3,\n",
       " 1.41,\n",
       " 3.8,\n",
       " 1.03,\n",
       " 3.41,\n",
       " 6.0,\n",
       " 1.0,\n",
       " 1.39,\n",
       " 2.36,\n",
       " 2.18,\n",
       " 1.3,\n",
       " 1.1,\n",
       " 1.87,\n",
       " 3.76,\n",
       " 1.52,\n",
       " 6.0,\n",
       " 2.08,\n",
       " 4.89,\n",
       " 5.59,\n",
       " 1.18,\n",
       " 6.0,\n",
       " 2.66,\n",
       " 1.65,\n",
       " 1.02,\n",
       " 1.59,\n",
       " 6.0,\n",
       " 1.28,\n",
       " 1.42,\n",
       " 1.08,\n",
       " 2.57,\n",
       " 1.02,\n",
       " 5.11,\n",
       " 1.96,\n",
       " 1.26,\n",
       " 1.17,\n",
       " 1.34,\n",
       " 1.05,\n",
       " 1.18,\n",
       " 2.99,\n",
       " 1.08,\n",
       " 2.19,\n",
       " 1.08,\n",
       " 1.14,\n",
       " 6.0,\n",
       " 2.29,\n",
       " 3.45,\n",
       " 1.07,\n",
       " 6.0,\n",
       " 1.19,\n",
       " 1.05,\n",
       " 1.67,\n",
       " 1.1,\n",
       " 6.0,\n",
       " 1.02,\n",
       " 1.53,\n",
       " 2.38,\n",
       " 1.38,\n",
       " 1.28,\n",
       " 5.21,\n",
       " 6.0,\n",
       " 1.17,\n",
       " 1.06,\n",
       " 3.69,\n",
       " 1.04,\n",
       " 1.54,\n",
       " 1.45,\n",
       " 1.47,\n",
       " 3.41,\n",
       " 6.0,\n",
       " 1.08,\n",
       " 2.15,\n",
       " 1.38,\n",
       " 6.0,\n",
       " 1.02,\n",
       " 4.11,\n",
       " 2.2,\n",
       " 4.17,\n",
       " 2.7,\n",
       " 1.04,\n",
       " 2.14,\n",
       " 1.37,\n",
       " 1.88,\n",
       " 2.85,\n",
       " 6.0,\n",
       " 1.09,\n",
       " 2.08,\n",
       " 4.32,\n",
       " 1.08,\n",
       " 5.52,\n",
       " 2.37,\n",
       " 2.47,\n",
       " 1.66,\n",
       " 1.3,\n",
       " 6.0,\n",
       " 2.6,\n",
       " 6.0,\n",
       " 1.65,\n",
       " 1.0,\n",
       " 1.88,\n",
       " 6.0,\n",
       " 1.51,\n",
       " 3.01,\n",
       " 2.11,\n",
       " 4.35,\n",
       " 1.14,\n",
       " 1.8,\n",
       " 5.07,\n",
       " 6.0,\n",
       " 2.43,\n",
       " 6.0,\n",
       " 1.97,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 3.93,\n",
       " 1.57,\n",
       " 1.31,\n",
       " 5.01,\n",
       " 3.79,\n",
       " 1.34,\n",
       " 5.43,\n",
       " 1.48,\n",
       " 1.26,\n",
       " 1.02,\n",
       " 1.63,\n",
       " 3.74,\n",
       " 1.16,\n",
       " 6.0,\n",
       " 1.29,\n",
       " 2.93,\n",
       " 6.0,\n",
       " 1.01,\n",
       " 6.0,\n",
       " 1.02,\n",
       " 2.73,\n",
       " 2.0,\n",
       " 1.11,\n",
       " 1.49,\n",
       " 4.1,\n",
       " 1.0,\n",
       " 5.61,\n",
       " 1.19,\n",
       " 1.03,\n",
       " 1.22,\n",
       " 1.27,\n",
       " 2.54,\n",
       " 1.36,\n",
       " 1.16,\n",
       " 1.14,\n",
       " 2.85,\n",
       " 1.38,\n",
       " 1.36,\n",
       " 6.0,\n",
       " 1.28,\n",
       " 1.25,\n",
       " 2.47,\n",
       " 3.92,\n",
       " 2.87,\n",
       " 1.09,\n",
       " 1.02,\n",
       " 1.59,\n",
       " 1.72,\n",
       " 1.28,\n",
       " 1.19,\n",
       " 3.48,\n",
       " 1.79,\n",
       " 1.0,\n",
       " 1.08,\n",
       " 3.69,\n",
       " 1.32,\n",
       " 6.0,\n",
       " 1.42,\n",
       " 1.0,\n",
       " 4.26,\n",
       " 1.37,\n",
       " 4.83,\n",
       " 1.65,\n",
       " 1.36,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 2.95,\n",
       " 2.53,\n",
       " 6.0,\n",
       " 1.42,\n",
       " 1.65,\n",
       " 4.93,\n",
       " 1.3,\n",
       " 1.76,\n",
       " 1.21,\n",
       " 4.58,\n",
       " 1.84,\n",
       " 3.43,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 1.61,\n",
       " 1.16,\n",
       " 4.51,\n",
       " 6.0,\n",
       " 2.75,\n",
       " 6.0,\n",
       " 1.65,\n",
       " 1.48,\n",
       " 1.53,\n",
       " 1.09,\n",
       " 1.12,\n",
       " 1.89,\n",
       " 3.44,\n",
       " 5.54,\n",
       " 1.37,\n",
       " 6.0,\n",
       " 3.48,\n",
       " 6.0,\n",
       " 1.07,\n",
       " 1.53,\n",
       " 1.14,\n",
       " 2.26,\n",
       " 1.01,\n",
       " 1.08,\n",
       " 1.01,\n",
       " 1.87,\n",
       " 1.04,\n",
       " 1.87,\n",
       " 3.55,\n",
       " 6.0,\n",
       " 1.05,\n",
       " 2.15,\n",
       " 1.54,\n",
       " 1.06,\n",
       " 1.35,\n",
       " 1.14,\n",
       " 2.66,\n",
       " 2.07,\n",
       " 1.37,\n",
       " 3.49,\n",
       " 1.09,\n",
       " 5.71,\n",
       " 1.85,\n",
       " 6.0,\n",
       " 1.5,\n",
       " 1.58,\n",
       " 2.32,\n",
       " 1.82,\n",
       " 3.7,\n",
       " 1.01,\n",
       " 6.0,\n",
       " 2.69,\n",
       " 3.86,\n",
       " 1.37,\n",
       " 1.74,\n",
       " 2.34,\n",
       " 1.75,\n",
       " 1.95,\n",
       " 1.25,\n",
       " 6.0,\n",
       " 6.0,\n",
       " 4.43,\n",
       " 2.03,\n",
       " 4.2,\n",
       " 6.0,\n",
       " 1.0,\n",
       " 1.08,\n",
       " 1.65,\n",
       " 1.09,\n",
       " 3.88,\n",
       " 3.35,\n",
       " 1.34,\n",
       " 1.64,\n",
       " 2.08,\n",
       " 2.59,\n",
       " 1.1,\n",
       " 1.08,\n",
       " 3.16,\n",
       " 6.0,\n",
       " 1.39,\n",
       " 6.0,\n",
       " 2.74,\n",
       " 3.18,\n",
       " 1.22,\n",
       " 1.12,\n",
       " 1.0,\n",
       " 1.5,\n",
       " 6.0,\n",
       " 1.73,\n",
       " 1.29,\n",
       " 6.0,\n",
       " 1.59,\n",
       " 1.55,\n",
       " 1.21,\n",
       " 1.7,\n",
       " 1.66,\n",
       " 4.99,\n",
       " 1.02,\n",
       " 1.73,\n",
       " 1.19,\n",
       " 2.93,\n",
       " 1.35,\n",
       " 1.07,\n",
       " 2.48,\n",
       " 1.12,\n",
       " 1.28,\n",
       " 1.96,\n",
       " 4.83,\n",
       " 1.2,\n",
       " 1.06,\n",
       " 1.09,\n",
       " 1.04,\n",
       " 2.14,\n",
       " 6.0,\n",
       " 2.36,\n",
       " 1.22,\n",
       " 6.0,\n",
       " 1.19,\n",
       " 1.17,\n",
       " 3.17,\n",
       " 2.85,\n",
       " 1.66,\n",
       " 2.03,\n",
       " 3.91,\n",
       " 4.78,\n",
       " 2.15,\n",
       " 1.23,\n",
       " 1.39,\n",
       " 2.1,\n",
       " 6.0,\n",
       " 2.43,\n",
       " 2.0,\n",
       " 1.12,\n",
       " 1.95,\n",
       " 1.0,\n",
       " 6.0,\n",
       " 1.0,\n",
       " 1.25,\n",
       " 1.58,\n",
       " 1.75,\n",
       " 1.81,\n",
       " 6.0,\n",
       " 3.14,\n",
       " 5.62,\n",
       " 1.37,\n",
       " 6.0,\n",
       " 1.0,\n",
       " 1.51,\n",
       " 1.43,\n",
       " 2.12,\n",
       " 1.33,\n",
       " 1.6,\n",
       " 1.06,\n",
       " 1.05,\n",
       " 1.01,\n",
       " 1.08,\n",
       " 1.67,\n",
       " 1.96,\n",
       " 5.43,\n",
       " 1.21,\n",
       " 1.11,\n",
       " 6.0,\n",
       " 2.13,\n",
       " 1.04,\n",
       " 2.98,\n",
       " 2.78,\n",
       " 1.87,\n",
       " 1.66,\n",
       " 1.09,\n",
       " 1.83,\n",
       " 1.7,\n",
       " 6.0,\n",
       " 1.35,\n",
       " 1.47,\n",
       " 1.01,\n",
       " 1.09,\n",
       " 1.21,\n",
       " 1.87,\n",
       " 1.51,\n",
       " 1.59,\n",
       " 2.17,\n",
       " 4.36,\n",
       " 3.61,\n",
       " 1.3,\n",
       " 1.06,\n",
       " 1.17,\n",
       " 1.11,\n",
       " 4.19,\n",
       " 1.78,\n",
       " 1.71,\n",
       " 1.12,\n",
       " 2.36,\n",
       " 1.71,\n",
       " 6.0,\n",
       " 1.2,\n",
       " 4.29,\n",
       " 1.23,\n",
       " 1.67,\n",
       " 1.02,\n",
       " 2.33,\n",
       " 1.0,\n",
       " 6.0,\n",
       " 3.95,\n",
       " 1.77,\n",
       " 1.33,\n",
       " 6.0,\n",
       " 2.49,\n",
       " 2.24,\n",
       " 6.0,\n",
       " 3.22,\n",
       " 1.49]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array3 = []\n",
    "array4 = []\n",
    "for i in range(600):\n",
    "    odd = array2[i].replace(',','.')\n",
    "    if float(odd) >= 6:\n",
    "        odd = 6\n",
    "    array3.append(float(odd))\n",
    "    if float(odd) >= 3:\n",
    "        corte1 = 1\n",
    "    else:\n",
    "        corte1 = 0\n",
    "    array4.append(corte1)\n",
    "\n",
    "array3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.  , 2.02, 2.02, ..., 1.73, 1.05, 1.52],\n",
       "       [2.02, 2.02, 1.54, ..., 1.05, 1.52, 5.63],\n",
       "       [2.02, 1.54, 2.2 , ..., 1.52, 5.63, 4.44],\n",
       "       ...,\n",
       "       [2.12, 1.33, 1.6 , ..., 2.49, 2.24, 6.  ],\n",
       "       [1.33, 1.6 , 1.06, ..., 2.24, 6.  , 3.22],\n",
       "       [1.6 , 1.06, 1.05, ..., 6.  , 3.22, 1.49]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matriz2 = matriz(60, array3)\n",
    "matriz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 1, 1],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 1, 1],\n",
       "       [0, 0, 0, ..., 1, 1, 0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matriz3 = matriz(60,array4)\n",
    "matriz3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 18,\n",
       " 17,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 15,\n",
       " 15,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 13,\n",
       " 13,\n",
       " 12,\n",
       " 11,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 12,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 12,\n",
       " 12,\n",
       " 13,\n",
       " 13,\n",
       " 14,\n",
       " 14,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 16,\n",
       " 16,\n",
       " 15,\n",
       " 16,\n",
       " 16,\n",
       " 17,\n",
       " 17,\n",
       " 16,\n",
       " 15,\n",
       " 14,\n",
       " 13,\n",
       " 13,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 15,\n",
       " 15,\n",
       " 14,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 20,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 20,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 22,\n",
       " 22,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 23,\n",
       " 23,\n",
       " 24,\n",
       " 24,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 27,\n",
       " 28,\n",
       " 27,\n",
       " 27,\n",
       " 27,\n",
       " 27,\n",
       " 28,\n",
       " 28,\n",
       " 27,\n",
       " 28,\n",
       " 28,\n",
       " 27,\n",
       " 28,\n",
       " 28,\n",
       " 27,\n",
       " 27,\n",
       " 28,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 29,\n",
       " 28,\n",
       " 27,\n",
       " 26,\n",
       " 25,\n",
       " 25,\n",
       " 24,\n",
       " 25,\n",
       " 24,\n",
       " 24,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 24,\n",
       " 24,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 24,\n",
       " 24,\n",
       " 23,\n",
       " 22,\n",
       " 21,\n",
       " 20,\n",
       " 20,\n",
       " 19,\n",
       " 18,\n",
       " 17,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 15,\n",
       " 16,\n",
       " 15,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 15,\n",
       " 16,\n",
       " 16,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 16,\n",
       " 15,\n",
       " 16,\n",
       " 15,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 15,\n",
       " 15,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 15,\n",
       " 14,\n",
       " 14,\n",
       " 15,\n",
       " 15,\n",
       " 16,\n",
       " 16,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 16,\n",
       " 16,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 18,\n",
       " 18,\n",
       " 19,\n",
       " 18,\n",
       " 19,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 19,\n",
       " 19,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 23,\n",
       " 23,\n",
       " 22,\n",
       " 23,\n",
       " 23,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 23,\n",
       " 23,\n",
       " 23,\n",
       " 22,\n",
       " 22,\n",
       " 23,\n",
       " 23,\n",
       " 23,\n",
       " 23,\n",
       " 22,\n",
       " 22,\n",
       " 21,\n",
       " 21,\n",
       " 22,\n",
       " 22,\n",
       " 23,\n",
       " 23,\n",
       " 23,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 21,\n",
       " 21,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 21,\n",
       " 20,\n",
       " 20,\n",
       " 19,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 19,\n",
       " 19,\n",
       " 18,\n",
       " 18,\n",
       " 17,\n",
       " 18,\n",
       " 18,\n",
       " 17,\n",
       " 16,\n",
       " 17,\n",
       " 16,\n",
       " 17,\n",
       " 16,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 16,\n",
       " 15,\n",
       " 14,\n",
       " 15,\n",
       " 15,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 17,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 15,\n",
       " 16,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 19,\n",
       " 18,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 21,\n",
       " 22,\n",
       " 22,\n",
       " 23,\n",
       " 23,\n",
       " 23,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 23,\n",
       " 23,\n",
       " 23,\n",
       " 22,\n",
       " 22,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 21,\n",
       " 20,\n",
       " 20,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 18,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 20,\n",
       " 20,\n",
       " 19,\n",
       " 19,\n",
       " 18,\n",
       " 17,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 18,\n",
       " 17,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 19,\n",
       " 18,\n",
       " 18,\n",
       " 17,\n",
       " 16,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 17,\n",
       " 18,\n",
       " 18,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 18,\n",
       " 18,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 20,\n",
       " 20,\n",
       " 19,\n",
       " 19,\n",
       " 18,\n",
       " 18,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 16,\n",
       " 16,\n",
       " 15,\n",
       " 15,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 17,\n",
       " 17,\n",
       " 16,\n",
       " 16,\n",
       " 15,\n",
       " 15,\n",
       " 14,\n",
       " 14,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 15,\n",
       " 14,\n",
       " 13,\n",
       " 13,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 14,\n",
       " 15,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 14,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 12,\n",
       " 12,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 12,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 12,\n",
       " 11,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 10,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 12]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array5 = []\n",
    "for i in range(len(array4) - 1):\n",
    "    if i >= 59:\n",
    "        order = sum(array4[i - 59: i])\n",
    "        array5.append(order)\n",
    "array5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[15, 14, 15, ..., 16, 16, 17],\n",
       "       [14, 15, 16, ..., 16, 17, 17],\n",
       "       [15, 16, 17, ..., 17, 17, 16],\n",
       "       ...,\n",
       "       [14, 14, 13, ..., 10, 11, 11],\n",
       "       [14, 13, 13, ..., 11, 11, 11],\n",
       "       [13, 13, 13, ..., 11, 11, 12]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matriz4 = matriz(60, array5)\n",
    "matriz4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indicador de futura função:\n",
    "import numpy as np\n",
    "\n",
    "def calculate_means(array4):\n",
    "    \"\"\"\n",
    "    Calcula a média dos elementos de array4 em janelas deslizantes de 59 elementos.\n",
    "\n",
    "    Args:\n",
    "        array4 (list): Lista de inteiros (0 ou 1).\n",
    "\n",
    "    Returns:\n",
    "        list: Lista com a média dos elementos em janelas de 59 elementos.\n",
    "    \"\"\"\n",
    "    array6 = []\n",
    "    array7 = []\n",
    "    for i in range(len(array4) - 1):\n",
    "        array6.append(array4[i])\n",
    "        if i >= 59:\n",
    "            order = float(np.mean(array6))\n",
    "            array7.append(order)\n",
    "    return array7\n",
    "\n",
    "# Exemplo de uso:\n",
    "# array4 = [0, 1, 0, 1, ...]  # Supondo que array4 tenha elementos suficientes\n",
    "# array7 = calculate_means(array4)\n",
    "\n",
    "# Teste unitário básico\n",
    "def test_calculate_means():\n",
    "    array4 = [1] * 60  # Lista com 60 elementos, todos iguais a 1\n",
    "    expected_output = [1.0]  # A média dos primeiros 59 elementos é 1.0\n",
    "    array7 = calculate_means(array4)\n",
    "    assert array7 == expected_output, \"Teste falhou!\"\n",
    "\n",
    "# Executar o teste\n",
    "test_calculate_means()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.25,\n",
       " 0.26229508196721313,\n",
       " 0.27419354838709675,\n",
       " 0.2857142857142857,\n",
       " 0.296875,\n",
       " 0.2923076923076923,\n",
       " 0.2878787878787879,\n",
       " 0.2835820895522388,\n",
       " 0.27941176470588236,\n",
       " 0.2753623188405797,\n",
       " 0.2714285714285714,\n",
       " 0.2676056338028169,\n",
       " 0.2638888888888889,\n",
       " 0.2602739726027397,\n",
       " 0.2702702702702703,\n",
       " 0.26666666666666666,\n",
       " 0.2631578947368421,\n",
       " 0.2597402597402597,\n",
       " 0.2564102564102564,\n",
       " 0.25316455696202533,\n",
       " 0.25,\n",
       " 0.24691358024691357,\n",
       " 0.24390243902439024,\n",
       " 0.25301204819277107,\n",
       " 0.25,\n",
       " 0.24705882352941178,\n",
       " 0.2558139534883721,\n",
       " 0.25287356321839083,\n",
       " 0.25,\n",
       " 0.24719101123595505,\n",
       " 0.25555555555555554,\n",
       " 0.25274725274725274,\n",
       " 0.25,\n",
       " 0.24731182795698925,\n",
       " 0.24468085106382978,\n",
       " 0.25263157894736843,\n",
       " 0.25,\n",
       " 0.25773195876288657,\n",
       " 0.25510204081632654,\n",
       " 0.25252525252525254,\n",
       " 0.25,\n",
       " 0.24752475247524752,\n",
       " 0.24509803921568626,\n",
       " 0.2524271844660194,\n",
       " 0.25,\n",
       " 0.24761904761904763,\n",
       " 0.24528301886792453,\n",
       " 0.2523364485981308,\n",
       " 0.25925925925925924,\n",
       " 0.26605504587155965,\n",
       " 0.2636363636363636,\n",
       " 0.26126126126126126,\n",
       " 0.25892857142857145,\n",
       " 0.26548672566371684,\n",
       " 0.2631578947368421,\n",
       " 0.2608695652173913,\n",
       " 0.2672413793103448,\n",
       " 0.26495726495726496,\n",
       " 0.2711864406779661,\n",
       " 0.2689075630252101,\n",
       " 0.26666666666666666,\n",
       " 0.2644628099173554,\n",
       " 0.26229508196721313,\n",
       " 0.2601626016260163,\n",
       " 0.25806451612903225,\n",
       " 0.264,\n",
       " 0.2619047619047619,\n",
       " 0.25984251968503935,\n",
       " 0.2578125,\n",
       " 0.2558139534883721,\n",
       " 0.25384615384615383,\n",
       " 0.2595419847328244,\n",
       " 0.25757575757575757,\n",
       " 0.2556390977443609,\n",
       " 0.26119402985074625,\n",
       " 0.25925925925925924,\n",
       " 0.25735294117647056,\n",
       " 0.26277372262773724,\n",
       " 0.2608695652173913,\n",
       " 0.2589928057553957,\n",
       " 0.2571428571428571,\n",
       " 0.2553191489361702,\n",
       " 0.2535211267605634,\n",
       " 0.2517482517482518,\n",
       " 0.25,\n",
       " 0.25517241379310346,\n",
       " 0.2602739726027397,\n",
       " 0.2653061224489796,\n",
       " 0.2702702702702703,\n",
       " 0.2751677852348993,\n",
       " 0.28,\n",
       " 0.2847682119205298,\n",
       " 0.28289473684210525,\n",
       " 0.2875816993464052,\n",
       " 0.2922077922077922,\n",
       " 0.2903225806451613,\n",
       " 0.28846153846153844,\n",
       " 0.2929936305732484,\n",
       " 0.2911392405063291,\n",
       " 0.2893081761006289,\n",
       " 0.29375,\n",
       " 0.2919254658385093,\n",
       " 0.29012345679012347,\n",
       " 0.2883435582822086,\n",
       " 0.2865853658536585,\n",
       " 0.28484848484848485,\n",
       " 0.28313253012048195,\n",
       " 0.2874251497005988,\n",
       " 0.2916666666666667,\n",
       " 0.2958579881656805,\n",
       " 0.3,\n",
       " 0.30409356725146197,\n",
       " 0.3081395348837209,\n",
       " 0.3063583815028902,\n",
       " 0.3103448275862069,\n",
       " 0.3142857142857143,\n",
       " 0.3181818181818182,\n",
       " 0.3220338983050847,\n",
       " 0.3202247191011236,\n",
       " 0.31843575418994413,\n",
       " 0.32222222222222224,\n",
       " 0.3259668508287293,\n",
       " 0.3241758241758242,\n",
       " 0.32786885245901637,\n",
       " 0.32608695652173914,\n",
       " 0.32432432432432434,\n",
       " 0.3225806451612903,\n",
       " 0.32085561497326204,\n",
       " 0.324468085106383,\n",
       " 0.32275132275132273,\n",
       " 0.32105263157894737,\n",
       " 0.32460732984293195,\n",
       " 0.3229166666666667,\n",
       " 0.32124352331606215,\n",
       " 0.3247422680412371,\n",
       " 0.3230769230769231,\n",
       " 0.32142857142857145,\n",
       " 0.3197969543147208,\n",
       " 0.32323232323232326,\n",
       " 0.32160804020100503,\n",
       " 0.325,\n",
       " 0.3283582089552239,\n",
       " 0.32673267326732675,\n",
       " 0.3251231527093596,\n",
       " 0.3235294117647059,\n",
       " 0.32195121951219513,\n",
       " 0.32038834951456313,\n",
       " 0.3188405797101449,\n",
       " 0.3173076923076923,\n",
       " 0.32057416267942584,\n",
       " 0.319047619047619,\n",
       " 0.3222748815165877,\n",
       " 0.32075471698113206,\n",
       " 0.323943661971831,\n",
       " 0.32710280373831774,\n",
       " 0.32558139534883723,\n",
       " 0.3287037037037037,\n",
       " 0.3271889400921659,\n",
       " 0.3256880733944954,\n",
       " 0.3242009132420091,\n",
       " 0.32272727272727275,\n",
       " 0.3257918552036199,\n",
       " 0.32432432432432434,\n",
       " 0.32286995515695066,\n",
       " 0.32142857142857145,\n",
       " 0.32,\n",
       " 0.3185840707964602,\n",
       " 0.32158590308370044,\n",
       " 0.3201754385964912,\n",
       " 0.31877729257641924,\n",
       " 0.3173913043478261,\n",
       " 0.31601731601731603,\n",
       " 0.3146551724137931,\n",
       " 0.3133047210300429,\n",
       " 0.31196581196581197,\n",
       " 0.31063829787234043,\n",
       " 0.3093220338983051,\n",
       " 0.3080168776371308,\n",
       " 0.3067226890756303,\n",
       " 0.30962343096234307,\n",
       " 0.30833333333333335,\n",
       " 0.3112033195020747,\n",
       " 0.30991735537190085,\n",
       " 0.31275720164609055,\n",
       " 0.3114754098360656,\n",
       " 0.31020408163265306,\n",
       " 0.3089430894308943,\n",
       " 0.3076923076923077,\n",
       " 0.31048387096774194,\n",
       " 0.3092369477911647,\n",
       " 0.308,\n",
       " 0.30677290836653387,\n",
       " 0.3055555555555556,\n",
       " 0.30434782608695654,\n",
       " 0.30708661417322836,\n",
       " 0.30980392156862746,\n",
       " 0.30859375,\n",
       " 0.30739299610894943,\n",
       " 0.31007751937984496,\n",
       " 0.3088803088803089,\n",
       " 0.3076923076923077,\n",
       " 0.3065134099616858,\n",
       " 0.3053435114503817,\n",
       " 0.30798479087452474,\n",
       " 0.3106060606060606,\n",
       " 0.30943396226415093,\n",
       " 0.3082706766917293,\n",
       " 0.30711610486891383,\n",
       " 0.30970149253731344,\n",
       " 0.30855018587360594,\n",
       " 0.3111111111111111,\n",
       " 0.30996309963099633,\n",
       " 0.3125,\n",
       " 0.31135531135531136,\n",
       " 0.3102189781021898,\n",
       " 0.3090909090909091,\n",
       " 0.3079710144927536,\n",
       " 0.30685920577617326,\n",
       " 0.3057553956834532,\n",
       " 0.30824372759856633,\n",
       " 0.30714285714285716,\n",
       " 0.30604982206405695,\n",
       " 0.30851063829787234,\n",
       " 0.30742049469964666,\n",
       " 0.30985915492957744,\n",
       " 0.3087719298245614,\n",
       " 0.3076923076923077,\n",
       " 0.30662020905923343,\n",
       " 0.3055555555555556,\n",
       " 0.3079584775086505,\n",
       " 0.30689655172413793,\n",
       " 0.30927835051546393,\n",
       " 0.3082191780821918,\n",
       " 0.30716723549488056,\n",
       " 0.30612244897959184,\n",
       " 0.30847457627118646,\n",
       " 0.30743243243243246,\n",
       " 0.30976430976430974,\n",
       " 0.3087248322147651,\n",
       " 0.3110367892976589,\n",
       " 0.31,\n",
       " 0.3089700996677741,\n",
       " 0.31125827814569534,\n",
       " 0.31353135313531355,\n",
       " 0.3125,\n",
       " 0.31475409836065577,\n",
       " 0.3137254901960784,\n",
       " 0.31596091205211724,\n",
       " 0.3181818181818182,\n",
       " 0.32038834951456313,\n",
       " 0.3193548387096774,\n",
       " 0.3183279742765273,\n",
       " 0.32051282051282054,\n",
       " 0.3226837060702875,\n",
       " 0.321656050955414,\n",
       " 0.3238095238095238,\n",
       " 0.3227848101265823,\n",
       " 0.3217665615141956,\n",
       " 0.32075471698113206,\n",
       " 0.31974921630094044,\n",
       " 0.321875,\n",
       " 0.32087227414330216,\n",
       " 0.32298136645962733,\n",
       " 0.3219814241486068,\n",
       " 0.32098765432098764,\n",
       " 0.3230769230769231,\n",
       " 0.3220858895705521,\n",
       " 0.3241590214067278,\n",
       " 0.3231707317073171,\n",
       " 0.3221884498480243,\n",
       " 0.3212121212121212,\n",
       " 0.3202416918429003,\n",
       " 0.3192771084337349,\n",
       " 0.3213213213213213,\n",
       " 0.3203592814371258,\n",
       " 0.32238805970149254,\n",
       " 0.32142857142857145,\n",
       " 0.32047477744807124,\n",
       " 0.31952662721893493,\n",
       " 0.3185840707964602,\n",
       " 0.3176470588235294,\n",
       " 0.31671554252199413,\n",
       " 0.3157894736842105,\n",
       " 0.31486880466472306,\n",
       " 0.313953488372093,\n",
       " 0.3130434782608696,\n",
       " 0.31213872832369943,\n",
       " 0.31412103746397696,\n",
       " 0.3132183908045977,\n",
       " 0.3123209169054441,\n",
       " 0.31142857142857144,\n",
       " 0.31339031339031337,\n",
       " 0.3125,\n",
       " 0.311614730878187,\n",
       " 0.3107344632768362,\n",
       " 0.30985915492957744,\n",
       " 0.3089887640449438,\n",
       " 0.3081232492997199,\n",
       " 0.30726256983240224,\n",
       " 0.30919220055710306,\n",
       " 0.30833333333333335,\n",
       " 0.3074792243767313,\n",
       " 0.30662983425414364,\n",
       " 0.3085399449035813,\n",
       " 0.3076923076923077,\n",
       " 0.3095890410958904,\n",
       " 0.3087431693989071,\n",
       " 0.3079019073569482,\n",
       " 0.30978260869565216,\n",
       " 0.3089430894308943,\n",
       " 0.3108108108108108,\n",
       " 0.30997304582210244,\n",
       " 0.30913978494623656,\n",
       " 0.3109919571045576,\n",
       " 0.31283422459893045,\n",
       " 0.31466666666666665,\n",
       " 0.31382978723404253,\n",
       " 0.3129973474801061,\n",
       " 0.3148148148148148,\n",
       " 0.31398416886543534,\n",
       " 0.3131578947368421,\n",
       " 0.31496062992125984,\n",
       " 0.31413612565445026,\n",
       " 0.3133159268929504,\n",
       " 0.3125,\n",
       " 0.3142857142857143,\n",
       " 0.3134715025906736,\n",
       " 0.3152454780361757,\n",
       " 0.3170103092783505,\n",
       " 0.31876606683804626,\n",
       " 0.31794871794871793,\n",
       " 0.3171355498721228,\n",
       " 0.31887755102040816,\n",
       " 0.32061068702290074,\n",
       " 0.3197969543147208,\n",
       " 0.32151898734177214,\n",
       " 0.3207070707070707,\n",
       " 0.3198992443324937,\n",
       " 0.31909547738693467,\n",
       " 0.3182957393483709,\n",
       " 0.3175,\n",
       " 0.3167082294264339,\n",
       " 0.31840796019900497,\n",
       " 0.3200992555831266,\n",
       " 0.3193069306930693,\n",
       " 0.32098765432098764,\n",
       " 0.3226600985221675,\n",
       " 0.32432432432432434,\n",
       " 0.3235294117647059,\n",
       " 0.32273838630806845,\n",
       " 0.32195121951219513,\n",
       " 0.32116788321167883,\n",
       " 0.32038834951456313,\n",
       " 0.3196125907990315,\n",
       " 0.3188405797101449,\n",
       " 0.3180722891566265,\n",
       " 0.3173076923076923,\n",
       " 0.31654676258992803,\n",
       " 0.3181818181818182,\n",
       " 0.3198090692124105,\n",
       " 0.319047619047619,\n",
       " 0.3182897862232779,\n",
       " 0.3175355450236967,\n",
       " 0.31678486997635935,\n",
       " 0.3160377358490566,\n",
       " 0.31529411764705884,\n",
       " 0.3145539906103286,\n",
       " 0.31381733021077285,\n",
       " 0.3130841121495327,\n",
       " 0.3146853146853147,\n",
       " 0.313953488372093,\n",
       " 0.31554524361948955,\n",
       " 0.3148148148148148,\n",
       " 0.3163972286374134,\n",
       " 0.315668202764977,\n",
       " 0.31494252873563217,\n",
       " 0.31422018348623854,\n",
       " 0.3135011441647597,\n",
       " 0.3150684931506849,\n",
       " 0.3143507972665148,\n",
       " 0.3159090909090909,\n",
       " 0.31519274376417233,\n",
       " 0.3167420814479638,\n",
       " 0.3160270880361174,\n",
       " 0.3153153153153153,\n",
       " 0.3146067415730337,\n",
       " 0.31390134529147984,\n",
       " 0.3131991051454139,\n",
       " 0.3125,\n",
       " 0.31403118040089084,\n",
       " 0.31555555555555553,\n",
       " 0.3170731707317073,\n",
       " 0.3163716814159292,\n",
       " 0.31788079470198677,\n",
       " 0.31938325991189426,\n",
       " 0.31868131868131866,\n",
       " 0.31798245614035087,\n",
       " 0.3172866520787746,\n",
       " 0.3165938864628821,\n",
       " 0.31808278867102396,\n",
       " 0.31956521739130433,\n",
       " 0.3188720173535792,\n",
       " 0.3181818181818182,\n",
       " 0.3174946004319654,\n",
       " 0.3168103448275862,\n",
       " 0.3161290322580645,\n",
       " 0.315450643776824,\n",
       " 0.3169164882226981,\n",
       " 0.31837606837606836,\n",
       " 0.31769722814498935,\n",
       " 0.3191489361702128,\n",
       " 0.3184713375796178,\n",
       " 0.3199152542372881,\n",
       " 0.3192389006342495,\n",
       " 0.31856540084388185,\n",
       " 0.3178947368421053,\n",
       " 0.3172268907563025,\n",
       " 0.31865828092243187,\n",
       " 0.3179916317991632,\n",
       " 0.3173277661795407,\n",
       " 0.31875,\n",
       " 0.3180873180873181,\n",
       " 0.31742738589211617,\n",
       " 0.3167701863354037,\n",
       " 0.31611570247933884,\n",
       " 0.3154639175257732,\n",
       " 0.3168724279835391,\n",
       " 0.3162217659137577,\n",
       " 0.3155737704918033,\n",
       " 0.3149284253578732,\n",
       " 0.3142857142857143,\n",
       " 0.3136456211812627,\n",
       " 0.3130081300813008,\n",
       " 0.31237322515212984,\n",
       " 0.3117408906882591,\n",
       " 0.3111111111111111,\n",
       " 0.31048387096774194,\n",
       " 0.3118712273641851,\n",
       " 0.3112449799196787,\n",
       " 0.3106212424849699,\n",
       " 0.31,\n",
       " 0.3093812375249501,\n",
       " 0.30876494023904383,\n",
       " 0.3101391650099404,\n",
       " 0.30952380952380953,\n",
       " 0.3089108910891089,\n",
       " 0.3102766798418972,\n",
       " 0.3096646942800789,\n",
       " 0.3090551181102362,\n",
       " 0.3104125736738703,\n",
       " 0.30980392156862746,\n",
       " 0.30919765166340507,\n",
       " 0.30859375,\n",
       " 0.30994152046783624,\n",
       " 0.311284046692607,\n",
       " 0.3106796116504854,\n",
       " 0.31007751937984496,\n",
       " 0.30947775628626695,\n",
       " 0.3088803088803089,\n",
       " 0.31021194605009633,\n",
       " 0.3096153846153846,\n",
       " 0.30902111324376197,\n",
       " 0.30842911877394635,\n",
       " 0.3078393881453155,\n",
       " 0.30725190839694655,\n",
       " 0.30857142857142855,\n",
       " 0.30798479087452474,\n",
       " 0.30740037950664134,\n",
       " 0.3068181818181818,\n",
       " 0.30623818525519847,\n",
       " 0.30566037735849055,\n",
       " 0.3069679849340866,\n",
       " 0.3082706766917293,\n",
       " 0.30956848030018763,\n",
       " 0.3089887640449438,\n",
       " 0.3102803738317757,\n",
       " 0.30970149253731344,\n",
       " 0.3091247672253259,\n",
       " 0.30855018587360594,\n",
       " 0.3079777365491651,\n",
       " 0.3074074074074074,\n",
       " 0.3068391866913124,\n",
       " 0.3062730627306273,\n",
       " 0.30570902394106814,\n",
       " 0.30514705882352944,\n",
       " 0.30458715596330277,\n",
       " 0.304029304029304,\n",
       " 0.30347349177330896,\n",
       " 0.30474452554744524,\n",
       " 0.30418943533697634,\n",
       " 0.30363636363636365,\n",
       " 0.30490018148820325,\n",
       " 0.30434782608695654,\n",
       " 0.3037974683544304,\n",
       " 0.30324909747292417,\n",
       " 0.3027027027027027,\n",
       " 0.302158273381295,\n",
       " 0.3016157989228007,\n",
       " 0.3010752688172043,\n",
       " 0.3005366726296959,\n",
       " 0.3,\n",
       " 0.30124777183600715,\n",
       " 0.30071174377224197,\n",
       " 0.30017761989342806,\n",
       " 0.299645390070922,\n",
       " 0.2991150442477876,\n",
       " 0.29858657243816256,\n",
       " 0.2980599647266314,\n",
       " 0.2975352112676056,\n",
       " 0.29701230228471004,\n",
       " 0.2964912280701754,\n",
       " 0.29772329246935203,\n",
       " 0.29895104895104896,\n",
       " 0.29842931937172773,\n",
       " 0.2979094076655052,\n",
       " 0.29739130434782607,\n",
       " 0.296875,\n",
       " 0.29809358752166376,\n",
       " 0.2975778546712803,\n",
       " 0.2970639032815199,\n",
       " 0.296551724137931,\n",
       " 0.29604130808950085,\n",
       " 0.29553264604810997,\n",
       " 0.2967409948542024,\n",
       " 0.2962328767123288,\n",
       " 0.29743589743589743,\n",
       " 0.29692832764505117,\n",
       " 0.29642248722316866,\n",
       " 0.29591836734693877,\n",
       " 0.29541595925297115,\n",
       " 0.29491525423728815,\n",
       " 0.2961082910321489,\n",
       " 0.2972972972972973,\n",
       " 0.29679595278246207,\n",
       " 0.2962962962962963,\n",
       " 0.29747899159663865,\n",
       " 0.29697986577181207,\n",
       " 0.2964824120603015,\n",
       " 0.2976588628762542,\n",
       " 0.2988313856427379]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array6 = []\n",
    "array7 = []\n",
    "for i in range(len(array4) - 1):\n",
    "    array6.append(array4[i])\n",
    "    if i >= 59:\n",
    "        order = float(np.mean(array6))\n",
    "        array7.append(order)\n",
    "\n",
    "array7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25      , 0.26229508, 0.27419355, ..., 0.26495726, 0.27118644,\n",
       "        0.26890756],\n",
       "       [0.26229508, 0.27419355, 0.28571429, ..., 0.27118644, 0.26890756,\n",
       "        0.26666667],\n",
       "       [0.27419355, 0.28571429, 0.296875  , ..., 0.26890756, 0.26666667,\n",
       "        0.26446281],\n",
       "       ...,\n",
       "       [0.30855019, 0.30797774, 0.30740741, ..., 0.29747899, 0.29697987,\n",
       "        0.29648241],\n",
       "       [0.30797774, 0.30740741, 0.30683919, ..., 0.29697987, 0.29648241,\n",
       "        0.29765886],\n",
       "       [0.30740741, 0.30683919, 0.30627306, ..., 0.29648241, 0.29765886,\n",
       "        0.29883139]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matriz5 = matriz(60, array7)\n",
    "matriz5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((541, 60), (541, 60), (481, 60), (481, 60))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matriz2.shape, matriz3.shape, matriz4.shape, matriz5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order = matriz2.shape[0] - matriz4.shape[0]\n",
    "order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrizfloat = matriz2[60:,:]\n",
    "matrizint = matriz3[60:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((481, 60), (481, 60), (481, 60), (481, 60))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrizfloat.shape, matrizint.shape, matriz4.shape, matriz5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1672, 600, 600, 540, 540)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(array2), len(array3), len(array4), len(array5), len(array7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = matrizfloat[:,:(matrizfloat.shape[1] - 1)]\n",
    "x2 = matriz4[:,:(matriz4.shape[1] - 1)]\n",
    "x3 = matriz5[:,:(matriz5.shape[1] - 1)]\n",
    "\n",
    "y = matrizint[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((481, 59), (481, 59), (481, 59), (481,))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.shape, x2.shape, x3.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59, 1443)\n"
     ]
    }
   ],
   "source": [
    "# Empilhar as matrizes para ter um eixo extra (60, 8, 3)\n",
    "X_stack = np.stack([x1, x2, x3], axis=2)  # Formato (60, 8, 3)\n",
    "\n",
    "# Reorganizar para intercalar coluna por coluna\n",
    "X_intercalado = X_stack.reshape(59, -1)  # Agora está no formato (60, 24)\n",
    "\n",
    "print(X_intercalado.shape)  # Saída: (60, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 5.63      , 15.        ,  0.25      , ...,  0.25641026,\n",
       "          1.77      , 14.        ],\n",
       "        [ 0.25316456,  1.02      , 13.        , ..., 14.        ,\n",
       "          0.25510204,  1.49      ],\n",
       "        [14.        ,  0.25252525,  1.94      , ...,  1.04      ,\n",
       "         16.        ,  0.27118644],\n",
       "        ...,\n",
       "        [ 1.6       , 13.        ,  0.30740741, ...,  0.30107527,\n",
       "          1.7       , 13.        ],\n",
       "        [ 0.30053667,  6.        , 13.        , ..., 12.        ,\n",
       "          0.29757785,  1.12      ],\n",
       "        [11.        ,  0.2970639 ,  2.36      , ...,  3.22      ,\n",
       "         11.        ,  0.29765886]]),\n",
       " array([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
       "        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "        0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
       "        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_intercalado, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Garantir que são arrays NumPy\n",
    "x1, x2, x3 = np.array(x1), np.array(x2), np.array(x3)\n",
    "\n",
    "# Criar DataFrames separando cada coluna\n",
    "df_x1 = pd.DataFrame(x1, columns=[f'X1_{i}' for i in range(x1.shape[1])])\n",
    "df_x2 = pd.DataFrame(x2, columns=[f'X2_{i}' for i in range(x2.shape[1])])\n",
    "df_x3 = pd.DataFrame(x3, columns=[f'X3_{i}' for i in range(x3.shape[1])])\n",
    "\n",
    "# Juntar todas as colunas\n",
    "X_df = pd.concat([df_x1, df_x2, df_x3], axis=1)\n",
    "\n",
    "# Transformar para valores NumPy\n",
    "X = X_df.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(481, 177) (481,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, y.shape)  # X deve ter o mesmo número de linhas de y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.63      , 4.44      , 3.91      , ..., 0.26724138, 0.26495726,\n",
       "        0.27118644],\n",
       "       [4.44      , 3.91      , 6.        , ..., 0.26495726, 0.27118644,\n",
       "        0.26890756],\n",
       "       [3.91      , 6.        , 2.44      , ..., 0.27118644, 0.26890756,\n",
       "        0.26666667],\n",
       "       ...,\n",
       "       [2.12      , 1.33      , 1.6       , ..., 0.2962963 , 0.29747899,\n",
       "        0.29697987],\n",
       "       [1.33      , 1.6       , 1.06      , ..., 0.29747899, 0.29697987,\n",
       "        0.29648241],\n",
       "       [1.6       , 1.06      , 1.05      , ..., 0.29697987, 0.29648241,\n",
       "        0.29765886]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia do modelo: 0.6701\n",
      "F1-Score do modelo: 0.5631\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Dividir os dados em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Criar e treinar o modelo XGBoost\n",
    "model = xgb.XGBClassifier(\n",
    "    objective='multi:softmax',  # Classificação multiclasse\n",
    "    num_class=2,  # Número de categorias na saída\n",
    "    eval_metric='mlogloss',\n",
    "    learning_rate=0.01,\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Fazer previsões\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Avaliar o modelo\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f'Acurácia do modelo: {accuracy:.4f}')\n",
    "print(f'F1-Score do modelo: {f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-04 17:30:25.719648: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-04 17:30:25.836661: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-04 17:30:25.942496: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741123826.052265   14195 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741123826.077160   14195 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-04 17:30:26.317913: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/darkcover/Documentos/Out/venv/lib/python3.12/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "/home/darkcover/Documentos/Out/venv/lib/python3.12/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.13.0 and strictly below 2.16.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.18.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "#activation = tf.keras.activations.elu\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "\n",
    "import skfuzzy as fuzz\n",
    "\n",
    "# Libs\n",
    "import time\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reden(array1, array3, m, n):\n",
    "    \"\"\"\n",
    "    Função para treinar uma rede neural usando as entradas e saídas fornecidas.\n",
    "\n",
    "    Args:\n",
    "        array1 (numpy.array): Saídas (rótulos) binárias (0 ou 1).\n",
    "        array3 (numpy.array): Entradas preditoras.\n",
    "        m (int): Número de amostras.\n",
    "        n (int): Número de características por amostra.\n",
    "\n",
    "    Returns:\n",
    "        keras.Model: Modelo treinado.\n",
    "    \"\"\"\n",
    "    # Dividindo os dados em treino e teste\n",
    "    X = np.array(array3)\n",
    "    y = np.array(array1)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Ajustando dimensões para entrada no modelo\n",
    "    x_train = np.expand_dims(x_train, -1).astype(\"float32\")\n",
    "    x_test = np.expand_dims(x_test, -1).astype(\"float32\")\n",
    "    input_shape = (n , 1)  # Formato esperado de entrada\n",
    "\n",
    "    # Convertendo saídas para categóricas\n",
    "    num_classes = 2\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "    # Definição do modelo\n",
    "    model = keras.Sequential([\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation=\"relu\", use_bias=True),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(64, activation=\"relu\", use_bias=True),\n",
    "        layers.Dense(32, activation=tf.keras.activations.swish, use_bias=True),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        loss=tfa.losses.SigmoidFocalCrossEntropy(alpha=0.7, gamma=2.0),\n",
    "        #loss=tfa.losses.SigmoidFocalCrossEntropy(alpha=0.25, gamma=2.0), #testa loss = tfa.losses.SigmoidFocalCrossEntropy(alpha=0.25, gamma=2.0)\n",
    "        optimizer=tf.keras.optimizers.AdamW(learning_rate=0.001, weight_decay=1e-4),\n",
    "        metrics=['accuracy', Precision(name=\"precision\"), Recall(name=\"recall\")]\n",
    "    )\n",
    "\n",
    "    # Treinamento\n",
    "    batch_size = 2**10\n",
    "    epochs = 50\n",
    "    model.fit(\n",
    "        x_train, y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_split=0.2,\n",
    "    )\n",
    "\n",
    "    # Avaliação\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print(f\"Test loss: {score[0]:.4f}\")\n",
    "    print(f\"Test accuracy: {score[1]:.4f}\")\n",
    "    print(f\"Precision: {score[2]:.4f}\")\n",
    "    print(f\"Recall: {score[3]:.4f}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
       "       0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
       "       1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(481, 177)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - accuracy: 0.5075 - loss: 2.7479 - precision: 0.5075 - recall: 0.5075 - val_accuracy: 0.7059 - val_loss: 3.2853 - val_precision: 0.7059 - val_recall: 0.7059\n",
      "Epoch 2/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.6343 - loss: 2.8283 - precision: 0.6343 - recall: 0.6343 - val_accuracy: 0.7059 - val_loss: 2.7800 - val_precision: 0.7059 - val_recall: 0.7059\n",
      "Epoch 3/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.6679 - loss: 2.3336 - precision: 0.6679 - recall: 0.6679 - val_accuracy: 0.7059 - val_loss: 1.3745 - val_precision: 0.7059 - val_recall: 0.7059\n",
      "Epoch 4/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 250ms/step - accuracy: 0.5784 - loss: 1.9375 - precision: 0.5784 - recall: 0.5784 - val_accuracy: 0.7353 - val_loss: 0.3892 - val_precision: 0.7353 - val_recall: 0.7353\n",
      "Epoch 5/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.5448 - loss: 1.9259 - precision: 0.5448 - recall: 0.5448 - val_accuracy: 0.5882 - val_loss: 0.3475 - val_precision: 0.5882 - val_recall: 0.5882\n",
      "Epoch 6/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.4590 - loss: 2.2392 - precision: 0.4590 - recall: 0.4590 - val_accuracy: 0.7353 - val_loss: 0.3811 - val_precision: 0.7353 - val_recall: 0.7353\n",
      "Epoch 7/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.5075 - loss: 1.7750 - precision: 0.5075 - recall: 0.5075 - val_accuracy: 0.7059 - val_loss: 0.6182 - val_precision: 0.7059 - val_recall: 0.7059\n",
      "Epoch 8/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.5448 - loss: 1.5854 - precision: 0.5448 - recall: 0.5448 - val_accuracy: 0.7059 - val_loss: 0.8214 - val_precision: 0.7059 - val_recall: 0.7059\n",
      "Epoch 9/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.5858 - loss: 1.2033 - precision: 0.5858 - recall: 0.5858 - val_accuracy: 0.7059 - val_loss: 0.8370 - val_precision: 0.7059 - val_recall: 0.7059\n",
      "Epoch 10/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.5709 - loss: 1.5434 - precision: 0.5709 - recall: 0.5709 - val_accuracy: 0.7059 - val_loss: 0.6720 - val_precision: 0.7059 - val_recall: 0.7059\n",
      "Epoch 11/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.6194 - loss: 0.9267 - precision: 0.6194 - recall: 0.6194 - val_accuracy: 0.7059 - val_loss: 0.4404 - val_precision: 0.7059 - val_recall: 0.7059\n",
      "Epoch 12/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.6157 - loss: 0.9634 - precision: 0.6157 - recall: 0.6157 - val_accuracy: 0.6912 - val_loss: 0.2622 - val_precision: 0.6912 - val_recall: 0.6912\n",
      "Epoch 13/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.5522 - loss: 0.8705 - precision: 0.5522 - recall: 0.5522 - val_accuracy: 0.6471 - val_loss: 0.1913 - val_precision: 0.6471 - val_recall: 0.6471\n",
      "Epoch 14/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.5821 - loss: 0.7536 - precision: 0.5821 - recall: 0.5821 - val_accuracy: 0.6618 - val_loss: 0.1744 - val_precision: 0.6618 - val_recall: 0.6618\n",
      "Epoch 15/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.5560 - loss: 0.7239 - precision: 0.5560 - recall: 0.5560 - val_accuracy: 0.6176 - val_loss: 0.1778 - val_precision: 0.6176 - val_recall: 0.6176\n",
      "Epoch 16/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.5112 - loss: 0.7424 - precision: 0.5112 - recall: 0.5112 - val_accuracy: 0.6324 - val_loss: 0.1808 - val_precision: 0.6324 - val_recall: 0.6324\n",
      "Epoch 17/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.5709 - loss: 0.5661 - precision: 0.5709 - recall: 0.5709 - val_accuracy: 0.5882 - val_loss: 0.1812 - val_precision: 0.5882 - val_recall: 0.5882\n",
      "Epoch 18/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.4925 - loss: 0.6641 - precision: 0.4925 - recall: 0.4925 - val_accuracy: 0.6324 - val_loss: 0.1757 - val_precision: 0.6324 - val_recall: 0.6324\n",
      "Epoch 19/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.5410 - loss: 0.5586 - precision: 0.5410 - recall: 0.5410 - val_accuracy: 0.6912 - val_loss: 0.1702 - val_precision: 0.6912 - val_recall: 0.6912\n",
      "Epoch 20/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.5672 - loss: 0.5508 - precision: 0.5672 - recall: 0.5672 - val_accuracy: 0.6618 - val_loss: 0.1670 - val_precision: 0.6618 - val_recall: 0.6618\n",
      "Epoch 21/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.5336 - loss: 0.4977 - precision: 0.5336 - recall: 0.5336 - val_accuracy: 0.6324 - val_loss: 0.1642 - val_precision: 0.6324 - val_recall: 0.6324\n",
      "Epoch 22/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.5821 - loss: 0.4447 - precision: 0.5821 - recall: 0.5821 - val_accuracy: 0.6471 - val_loss: 0.1630 - val_precision: 0.6471 - val_recall: 0.6471\n",
      "Epoch 23/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.5522 - loss: 0.4848 - precision: 0.5522 - recall: 0.5522 - val_accuracy: 0.5735 - val_loss: 0.1663 - val_precision: 0.5735 - val_recall: 0.5735\n",
      "Epoch 24/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.5672 - loss: 0.3844 - precision: 0.5672 - recall: 0.5672 - val_accuracy: 0.6029 - val_loss: 0.1692 - val_precision: 0.6029 - val_recall: 0.6029\n",
      "Epoch 25/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.5784 - loss: 0.3223 - precision: 0.5784 - recall: 0.5784 - val_accuracy: 0.5882 - val_loss: 0.1699 - val_precision: 0.5882 - val_recall: 0.5882\n",
      "Epoch 26/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step - accuracy: 0.5858 - loss: 0.2961 - precision: 0.5858 - recall: 0.5858 - val_accuracy: 0.5735 - val_loss: 0.1696 - val_precision: 0.5735 - val_recall: 0.5735\n",
      "Epoch 27/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step - accuracy: 0.5970 - loss: 0.3057 - precision: 0.5970 - recall: 0.5970 - val_accuracy: 0.5882 - val_loss: 0.1684 - val_precision: 0.5882 - val_recall: 0.5882\n",
      "Epoch 28/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.5299 - loss: 0.3247 - precision: 0.5299 - recall: 0.5299 - val_accuracy: 0.6176 - val_loss: 0.1675 - val_precision: 0.6176 - val_recall: 0.6176\n",
      "Epoch 29/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.5112 - loss: 0.3181 - precision: 0.5112 - recall: 0.5112 - val_accuracy: 0.6765 - val_loss: 0.1658 - val_precision: 0.6765 - val_recall: 0.6765\n",
      "Epoch 30/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.5112 - loss: 0.3256 - precision: 0.5112 - recall: 0.5112 - val_accuracy: 0.6618 - val_loss: 0.1658 - val_precision: 0.6618 - val_recall: 0.6618\n",
      "Epoch 31/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.5821 - loss: 0.2877 - precision: 0.5821 - recall: 0.5821 - val_accuracy: 0.6765 - val_loss: 0.1662 - val_precision: 0.6765 - val_recall: 0.6765\n",
      "Epoch 32/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.6082 - loss: 0.2704 - precision: 0.6082 - recall: 0.6082 - val_accuracy: 0.7059 - val_loss: 0.1657 - val_precision: 0.7059 - val_recall: 0.7059\n",
      "Epoch 33/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.5821 - loss: 0.2879 - precision: 0.5821 - recall: 0.5821 - val_accuracy: 0.7059 - val_loss: 0.1641 - val_precision: 0.7059 - val_recall: 0.7059\n",
      "Epoch 34/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.5821 - loss: 0.2549 - precision: 0.5821 - recall: 0.5821 - val_accuracy: 0.7059 - val_loss: 0.1622 - val_precision: 0.7059 - val_recall: 0.7059\n",
      "Epoch 35/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.5634 - loss: 0.2293 - precision: 0.5634 - recall: 0.5634 - val_accuracy: 0.7059 - val_loss: 0.1606 - val_precision: 0.7059 - val_recall: 0.7059\n",
      "Epoch 36/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.6157 - loss: 0.2348 - precision: 0.6157 - recall: 0.6157 - val_accuracy: 0.7059 - val_loss: 0.1595 - val_precision: 0.7059 - val_recall: 0.7059\n",
      "Epoch 37/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.6007 - loss: 0.2304 - precision: 0.6007 - recall: 0.6007 - val_accuracy: 0.7059 - val_loss: 0.1590 - val_precision: 0.7059 - val_recall: 0.7059\n",
      "Epoch 38/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.6045 - loss: 0.2132 - precision: 0.6045 - recall: 0.6045 - val_accuracy: 0.7059 - val_loss: 0.1592 - val_precision: 0.7059 - val_recall: 0.7059\n",
      "Epoch 39/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.5709 - loss: 0.2319 - precision: 0.5709 - recall: 0.5709 - val_accuracy: 0.6765 - val_loss: 0.1592 - val_precision: 0.6765 - val_recall: 0.6765\n",
      "Epoch 40/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.5634 - loss: 0.2277 - precision: 0.5634 - recall: 0.5634 - val_accuracy: 0.6912 - val_loss: 0.1596 - val_precision: 0.6912 - val_recall: 0.6912\n",
      "Epoch 41/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.5522 - loss: 0.2155 - precision: 0.5522 - recall: 0.5522 - val_accuracy: 0.7059 - val_loss: 0.1597 - val_precision: 0.7059 - val_recall: 0.7059\n",
      "Epoch 42/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.6269 - loss: 0.1995 - precision: 0.6269 - recall: 0.6269 - val_accuracy: 0.6912 - val_loss: 0.1594 - val_precision: 0.6912 - val_recall: 0.6912\n",
      "Epoch 43/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.5448 - loss: 0.2233 - precision: 0.5448 - recall: 0.5448 - val_accuracy: 0.6912 - val_loss: 0.1587 - val_precision: 0.6912 - val_recall: 0.6912\n",
      "Epoch 44/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.5896 - loss: 0.1945 - precision: 0.5896 - recall: 0.5896 - val_accuracy: 0.6618 - val_loss: 0.1582 - val_precision: 0.6618 - val_recall: 0.6618\n",
      "Epoch 45/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.5560 - loss: 0.2082 - precision: 0.5560 - recall: 0.5560 - val_accuracy: 0.6618 - val_loss: 0.1574 - val_precision: 0.6618 - val_recall: 0.6618\n",
      "Epoch 46/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.5560 - loss: 0.2015 - precision: 0.5560 - recall: 0.5560 - val_accuracy: 0.6618 - val_loss: 0.1567 - val_precision: 0.6618 - val_recall: 0.6618\n",
      "Epoch 47/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 284ms/step - accuracy: 0.5858 - loss: 0.2015 - precision: 0.5858 - recall: 0.5858 - val_accuracy: 0.6618 - val_loss: 0.1558 - val_precision: 0.6618 - val_recall: 0.6618\n",
      "Epoch 48/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.6157 - loss: 0.1901 - precision: 0.6157 - recall: 0.6157 - val_accuracy: 0.6765 - val_loss: 0.1549 - val_precision: 0.6765 - val_recall: 0.6765\n",
      "Epoch 49/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.5933 - loss: 0.1797 - precision: 0.5933 - recall: 0.5933 - val_accuracy: 0.6765 - val_loss: 0.1542 - val_precision: 0.6765 - val_recall: 0.6765\n",
      "Epoch 50/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.5784 - loss: 0.2014 - precision: 0.5784 - recall: 0.5784 - val_accuracy: 0.6912 - val_loss: 0.1536 - val_precision: 0.6912 - val_recall: 0.6912\n",
      "Test loss: 0.1560\n",
      "Test accuracy: 0.7172\n",
      "Precision: 0.7172\n",
      "Recall: 0.7172\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = reden(y, X, X.shape[0], X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((481, 177), (481,))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
